{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af912e0",
   "metadata": {},
   "source": [
    "## <b> Google ADK + AMD Instinct™ GPUs: The Dynamic Duo for AI Agents </b>\n",
    "\n",
    "This notebook walks through building a real-world Agent-to-Agent (A2A) system for a Purchasing Concierge, where multiple specialized agents (Root, Burger, Pizza) work together seamlessly.\n",
    "\n",
    "**Purchasing Agent (Google ADK + Ollama)** – Main Agent (aka root_agent which orchestrates conversations and routes user requests, using LiteLlm with a locally hosted Ollama model.  \n",
    "**Burger Seller Agent (CrewAI + vLLM)** – presents the burger menu, provides pricing, and handles order creation, powered by a vLLM-hosted model.  \n",
    "**Pizza Seller Agent (LangGraph + Ollama)** – specializes in pizza ordering, built with LangGraph on top of Ollama.\n",
    "\n",
    "The root agent coordinates orders by delegating tasks to these seller agents (Pizza and Burger) through the open A2A protocol, enabling seamless collaboration across frameworks.\n",
    "\n",
    "The system runs locally on AMD GPUs and includes an interactive Gradio UI, showcasing real-world agent interoperability and cross-framework integration.\n",
    "\n",
    "<img src=\"./assets/image.png\" alt=\"agents-architecture\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f431a2-cd3b-407d-afa2-71b576db1d1e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Use the following setup to run this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be695411-785b-4a38-a0bd-b694c7c515b0",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "\n",
    "For this tutorial, you'll need a system with an AMD Instinct GPU. To run the model on the CPU and use AMD ZenDNN, you need an AMD EPYC CPU. \n",
    "\n",
    "This tutorial was tested on the following hardware:\n",
    "* AMD Instinct MI100\n",
    "* AMD Instinct MI210\n",
    "* AMD Instinct MI300X\n",
    "* 4th generation AMD EPYC (Genoa)\n",
    "* 5th generation AMD EPYC (Turin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135ed61-bba4-4f0a-be93-dd7637517b0e",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04 or later.\n",
    "* **ROCm 6.3**: This is only required for GPU execution. Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "* **PyTorch 2.6** (or later)\n",
    "* **vLLM** serving a model of your choice in a Docker container (requires `sudo` rights)\n",
    "* **Ollama**: this will be used to serve another model of your choice\n",
    "  \n",
    "### Install and launch Jupyter Notebooks\n",
    "If Jupyter is not already installed on your system, install it and launch JupyterLab using the following commands:\n",
    "\n",
    "```\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "To start the Jupyter server, run the following command:\n",
    "\n",
    "```\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "After the command executes, the terminal output displays a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue to follow the steps in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a8024-3432-41be-9717-776919e75ba2",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "This Tutorial is divided into 5 Parts, namely:\n",
    "* **Part 1**: Some introduction to Agentic AI, covering the basics you will need to grasp concepts introduced in Part 2,3 and 4. This section introduces key concepts like A2A Protocol, Agent Cards, Sessions, Memory and brushes the basics of security and authentication.\n",
    "* **Part 2**: We build our first Agent - the Burger Seller Agent - using CrewAI, and see how it functions with an LLM (served by vLLM) at the heart of it\n",
    "* **Part 3**: We build our second Agent - the Pizza Seller Agent - using LangGraph, with an LLM served by Ollama\n",
    "* **Part 4**: We build our third and final Agent - the Purchasing Concierge Agent, built with Google ADK, and see who we stick all three agents together, like little lego blocks. \n",
    "* **Part 5**: We spawn a GradioUI instance and interact with our Purchasing Concierge in the GUI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd9cb6",
   "metadata": {},
   "source": [
    "# Part 1: A quick Primer to Agentic AI, Google ADK, and key concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c08e72",
   "metadata": {},
   "source": [
    "For **creating an A2A system**, the first thing we need is an **Agent**.  \n",
    "An agent is essentially a smart computer program designed to act on a human’s behalf, much like a personal assistant. The agents use **large language models (LLMs) as their \"brain\"**. It perceives inputs, reasons with them and acts to achieve goals.\n",
    "\n",
    "Once we have an agent, the next piece is an **Agent Card**.  \n",
    "This is a simple JSON metadata document that describes the agent’s identity, capabilities, and endpoint. This is more of the agent’s “business card”, it tells others who the agent is, what it can do, and how to interact with it. Agent Cards are the foundation of how agents discover and talk to each other in the A2A ecosystem.\n",
    "\n",
    "Next comes the **A2A Protocol**, which makes agents communicate and collaborate.  \n",
    "The A2A protocol is an open standard that provides a common language for agents built with different frameworks or by different vendors. It introduces two key roles:\n",
    "\n",
    "**A2A Client** – An application or agent that initiates a request on behalf of the user. In our project, the **Root Agent (ADK)** plays this role when it calls out to seller agents.  \n",
    "\n",
    "**A2A Server** – An agent or agentic system that exposes an HTTP endpoint to accept requests, process tasks, and return results. In our project, the **Burger Agent (CrewAI)** and **Pizza Agent (LangChain)** serve as A2A Servers.  \n",
    "\n",
    "These agents communicate by exchanging Tasks (a unit of work, e.g., ‘place a burger order’) made up of multiple Messages.\n",
    "\n",
    "Together, **Agents, Agent Cards, and the A2A Protocol (with Client/Server roles)** form the foundation for building interconnected agentic systems. With frameworks like the **Agent Development Kit (ADK)**, we can easily create our own agents and connect them into larger ecosystems where a Root Agent can discover, call, and coordinate remote agents running behind A2A servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb99a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **How AI Agents Keep Your Conversation Alive**\n",
    "\n",
    "Just like humans rely on memory to maintain coherent conversations, AI agents need a way to track context across multiple exchanges. Without this capability, each interaction would be isolated—imagine talking to someone who forgets everything you said the moment after you say it.\n",
    "\n",
    "The Agent Development Kit (ADK) solves this through three core components:\n",
    "- **Session** : Maintains the current conversation thread and keeps track of the ongoing conversation from start to finish, including every message exchanged and action taken during that specific chat.\n",
    "- **State** : Is your conversation's \"scratch pad\". It holds temporary data that's only relevant to your current conversation thread, like items you've added to a cart or preferences you've mentioned right now. This data gets cleared when the conversation ends.\n",
    "- **Memory** : Acts as a searchable knowledge base that spans across all your past sessions, letting the agent recall and reference information or context beyond the current conversation.\n",
    "\n",
    "These components work together to create agents that can reference previous conversations and build meaningful conversational context over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0efcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Security Considerations in Multi-Agent System**\n",
    "\n",
    "The A2A Protocol is designed with enterprise standards as its core, ensuring agents can collaborate securely and at scale. Instead of new standards, it builds on widely adopted enterprise practices.\n",
    "\n",
    "- **Secure**: All communication happens over HTTPS with modern TLS, certificate validation, and protection against eavesdropping or tampering.\n",
    "- **Authentication**: Each agent server enforces authentication using standard web methods (OAuth2, OpenID Connect, API keys), with credentials passed in HTTP headers and managed outside the protocol.\n",
    "- **Authorization**: Enforces granular, skill-based, and least-privilege access controls, ensuring agents only perform permitted actions.\n",
    "- **Observability**: Integrates with tracing, logging, and monitoring tools to provide visibility, debugging, and auditing.\n",
    "- **API Management**: Agent endpoints are discoverable via Agent Cards, supporting centralized governance and easy onboarding of new agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273702c6",
   "metadata": {},
   "source": [
    "### Notebook preparation\n",
    "\n",
    "To run this notebook locally, you will first need a bunch of \"utilities\" which include some definition of the Google A2A types, logic on resolving Agent Cards, and a few other files for Task Management. These utility files enable third party agentic frameworks that we use in this project (ex: *CrewAI* and *LangGraph*) to be compatible with the *Google A2A* protocol.\n",
    "\n",
    "We first download the utlities and move the cloned files one folder level above so that we can easily use them throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73463b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define the repository URL and branch\n",
    "REPO_URL=\"https://github.com/shailensobhee/google-adk-agentic-ai-tutorial.git\"\n",
    "BRANCH_NAME=\"utils\"\n",
    "REPO_FOLDER=\"google-adk-agentic-ai-tutorial\"\n",
    "\n",
    "# Check if the repository folder already exists\n",
    "if [ -d \"$REPO_FOLDER\" ]; then\n",
    "  echo \"The repository already exists. Updating it...\"\n",
    "  cd \"$REPO_FOLDER\"\n",
    "  git pull origin \"$BRANCH_NAME\"\n",
    "else\n",
    "  echo \"Cloning the repository for the first time...\"\n",
    "  git clone --single-branch --branch \"$BRANCH_NAME\" \"$REPO_URL\"\n",
    "fi\n",
    "cp -r $REPO_FOLDER/* .\n",
    "rm -rf $REPO_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49d5ac",
   "metadata": {},
   "source": [
    "We then install all required Python packages that we will need throughout the rest of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7b205-4b90-44fd-82b4-88b61b03a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f964d8",
   "metadata": {},
   "source": [
    "# Part 2: Building the first Agent (Burger Seller) with CrewAI and vLLM\n",
    "\n",
    "This agent is built with **CrewAI**, and its core **LLM** is served via **vLLM**. It presents the burger menu, provides pricing, and handles order creation through the A2A protocol. As we create this remote agent using CrewAI, the standard CrewAI agent structure is:\n",
    "\n",
    "***Agent → Task → Crew → Output***\n",
    "\n",
    "- **Agent**: The specialized entity (role, goal, backstory, tools, LLM).  \n",
    "- **Task**: The specific work the agent will perform, with detailed instructions and expected output format.  \n",
    "- **Crew**: The orchestrator that links agents and tasks together, defines process flow, and executes the workflow.  \n",
    "\n",
    "In CrewAI, tasks must always be linked to agents **through a Crew** in order to run.\n",
    "\n",
    "Additionally, in this example we’ll create a custom tool, `create_burger_order`, to handle deterministic order creation.\n",
    "\n",
    "Now, let’s implement the Burger agent step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2159ad-4889-457a-b5ad-c4787f44d192",
   "metadata": {},
   "source": [
    "### Step 1: Serve a vLLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e658d3-eda1-4d1b-ae37-145f37c4ef9e",
   "metadata": {},
   "source": [
    "To get started with serving a model of your choice with vLLM, you'll need to run a few commands in a separate terminal. The following steps will guide you through the process, which involves running a vLLM Docker container and serving a model with specific configurations for tool-calling agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964561f6-80ad-4a0e-b2c0-3137460da936",
   "metadata": {},
   "source": [
    "`docker run -d -p 8088:8088 -it --ipc=host --network=host --privileged --cap-add=CAP_SYS_ADMIN --device=/dev/kfd --device=/dev/dri --device=/dev/mem --group-add render --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -e HF_TOKEN=$HF_TOKEN --name vllm_server  rocm/vllm:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1df45b-1016-4c5a-9e38-9a0f0f121878",
   "metadata": {},
   "source": [
    "Then, attach to that container: `docker attach vllm_server`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60787223-a9dd-442a-aaf8-aadbbba1c0b6",
   "metadata": {},
   "source": [
    "#### Step1a: Serve the model with Tool-Calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec33f43-59bf-4d8f-89bc-8c475c46d447",
   "metadata": {},
   "source": [
    "Once inside the container, you can serve your model. The provided command uses a tool-call parser and a specific chat template, which are crucial for agentic AI applications that require structured function calls.\n",
    "\n",
    "Export your Hugging Face token to enable access to gated models.  \n",
    "`export HF_TOKEN=<your HF token>`  \n",
    "\n",
    "Run the vllm serve command to start the model. This example uses `meta-llama/Llama-3.1-8B-Instruct`, but you can replace it with any model you prefer.  \n",
    "`--enable-auto-tool-choice`: Activates the model's ability to automatically select the appropriate tool.  \n",
    "`--tool-call-parser llama3_json`: Specifies the parser for interpreting tool-calling output, which is crucial for handling structured JSON responses.  \n",
    "`--chat-template`: Points to the specific Jinja template file. This template formats the conversation history in a way that the model understands for generating tool-call responses.\n",
    "\n",
    "Finally, start the server as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6f016-1418-438d-a0c6-65be5984f7d3",
   "metadata": {},
   "source": [
    "`vllm serve meta-llama/Llama-3.1-8B-Instruct \\`  \n",
    "`--enable-auto-tool-choice \\`  \n",
    "`--tool-call-parser llama3_json \\`  \n",
    "`--port 8088 \\`  \n",
    "`-chat-template /path_to/vllm/examples/tool_chat_template_llama3.1_json.jinja`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f328f7d",
   "metadata": {},
   "source": [
    "### Step 2: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab557f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set these to the correct values for your setup\n",
    "os.environ[\"VLLM_MODEL\"] = \"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8088/v1\" # vLLM serve URL (we used port 8089 here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852aa3",
   "metadata": {},
   "source": [
    "### Step 3: Define a tool (`create_burger_order`)\n",
    "\n",
    "LLMs are great at **deciding** what to do, but tools are how they actually **do** it.  \n",
    "The `create_burger_order` tool turns the model’s intent into a deterministic, auditable action.\n",
    "\n",
    "The tool takes the burger items the user wants, generates a unique order ID, and builds a clean, structured order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b43ba4-9782-412f-b714-6e4be2221b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "from crewai.tools import tool\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    status: Literal[\"input_required\", \"completed\", \"error\"] = \"input_required\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: int\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    order_items: list[OrderItem]\n",
    "\n",
    "\n",
    "@tool(\"create_order\")\n",
    "def create_burger_order(order_items: list[OrderItem]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new burger order with the given order items.\n",
    "\n",
    "    Args:\n",
    "        order_items: List of order items to be added to the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the order has been created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        order_id = str(uuid.uuid4())\n",
    "        order = Order(order_id=order_id, status=\"created\", order_items=order_items)\n",
    "        print(\"===\")\n",
    "        print(f\"order created: {order}\")\n",
    "        print(\"===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating order: {e}\")\n",
    "        return f\"Error creating order: {e}\"\n",
    "    return f\"Order {order.model_dump()} has been created\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd3ee2",
   "metadata": {},
   "source": [
    "### Step 4: Define BurgerSellerAgent class  \n",
    "\n",
    "Previously we mentioned the standard CrewAI agent structure:\n",
    "\n",
    "***Agent → Task → Crew → Output***\n",
    "\n",
    "Now this BurgerSellerAgent class is essentially a wrapper around the Agent–Task–Crew pipeline. It constructs the Agent + Task + Crew. \n",
    "\n",
    "The class BurgerSellerAgent defines the core intelligence and business rules of the burger store agent. It:\n",
    "\n",
    "**- Encapsulates** the instructions, context, and rules the LLM must follow (menu, pricing, confirmation flow, error handling).  \n",
    "**- Connects** the LLM (via CrewAI) with the create_burger_order tool so that order creation is deterministic and safe.  \n",
    "**- Produces** a structured response format (ResponseFormat) that downstream systems (like the A2A server) can reliably consume.  \n",
    "**- Handles** different states (input_required, error, completed) and translates them into a uniform response for the server.  \n",
    "**- Acts** as the bridge between user queries and the A2A server, so that the server doesn’t need to know the conversation logic—it just hosts the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972af52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent as crewagent\n",
    "from crewai import Crew as crewcrew\n",
    "from crewai import LLM as crewllm\n",
    "from crewai import Task as crewtask\n",
    "from crewai import Process as crewprocess\n",
    "\n",
    "class BurgerSellerAgent:\n",
    "    TaskInstruction = \"\"\"\n",
    "# INSTRUCTIONS\n",
    "\n",
    "You are a specialized assistant for a burger store.\n",
    "Your sole purpose is to answer questions about what is available on burger menu and price also handle order creation.\n",
    "If the user asks about anything other than burger menu or order creation, politely state that you cannot help with that topic and can only assist with burger menu and order creation.\n",
    "Do not attempt to answer unrelated questions or use tools for other purposes.\n",
    "\n",
    "# CONTEXT\n",
    "\n",
    "Received user query: {user_prompt}\n",
    "Session ID: {session_id}\n",
    "\n",
    "Provided below is the available burger menu and it's related price:\n",
    "- Classic Cheeseburger: IDR 85K\n",
    "- Double Cheeseburger: IDR 110K\n",
    "- Spicy Chicken Burger: IDR 80K\n",
    "- Spicy Cajun Burger: IDR 85K\n",
    "\n",
    "# RULES\n",
    "\n",
    "- If user want to do something, you will be following this order:\n",
    "    1. Always ensure the user already confirmed the order and total price. This confirmation may already given in the user query.\n",
    "    2. Use `create_burger_order` tool to create the order\n",
    "    3. Finally, always provide response to the user about the detailed ordered items, price breakdown and total, and order ID\n",
    "    \n",
    "- Set response status to input_required if asking for user order confirmation.\n",
    "- Set response status to error if there is an error while processing the request.\n",
    "- Set response status to completed if the request is complete.\n",
    "- DO NOT make up menu or price, Always rely on the provided menu given to you as context.\n",
    "\"\"\"\n",
    "    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"]\n",
    "\n",
    "    def invoke(self, query, sessionId) -> str:\n",
    "        burger_agent = crewagent(\n",
    "            role=\"Burger Seller Agent\",\n",
    "            goal=(\n",
    "                \"Help user to understand what is available on burger menu and price also handle order creation.\"\n",
    "            ),\n",
    "            backstory=(\"You are an expert and helpful burger seller agent.\"),\n",
    "            verbose=False,\n",
    "            allow_delegation=False,\n",
    "            tools=[create_burger_order],\n",
    "            llm=crewllm(\n",
    "                model=\"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\", # os.getenv(\"VLLM_MODEL\"), #VLLM_MODEL\n",
    "                api_base=\"http://localhost:8088/v1\" # os.getenv(\"OPENAI_API_BASE\") # OPENAI_API_BASE\n",
    "                )\n",
    "        )\n",
    "\n",
    "        agent_task = crewtask(\n",
    "            description=self.TaskInstruction,\n",
    "            output_pydantic=ResponseFormat,\n",
    "            agent=burger_agent,\n",
    "            expected_output=(\n",
    "                \"A JSON object with 'status' and 'message' fields.\"\n",
    "                \"Set response status to input_required if asking for user order confirmation.\"\n",
    "                \"Set response status to error if there is an error while processing the request.\"\n",
    "                \"Set response status to completed if the request is complete.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        crew = crewcrew(\n",
    "            tasks=[agent_task],\n",
    "            agents=[burger_agent],\n",
    "            verbose=False,\n",
    "            process=crewprocess.sequential,\n",
    "        )\n",
    "\n",
    "        inputs = {\"user_prompt\": query, \"session_id\": sessionId}\n",
    "        response = crew.kickoff(inputs)\n",
    "        return self.get_agent_response(response)\n",
    "\n",
    "    def get_agent_response(self, response):\n",
    "        response_object = response.pydantic\n",
    "        if response_object and isinstance(response_object, ResponseFormat):\n",
    "            if response_object.status == \"input_required\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"error\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"completed\":\n",
    "                return {\n",
    "                    \"is_task_complete\": True,\n",
    "                    \"require_user_input\": False,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"is_task_complete\": False,\n",
    "            \"require_user_input\": True,\n",
    "            \"content\": \"We are unable to process your request at the moment. Please try again.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d8a4e",
   "metadata": {},
   "source": [
    "### Step 5: Start the Burger Agent A2A Server\n",
    "\n",
    "We now launch the **Burger Agent as an A2A server.**\n",
    "This wraps the CrewAI pipeline **(Agent → Task → Crew → Output)** inside the A2AServer, so the agent is exposed as a discoverable **A2A service**.\n",
    "\n",
    "Running it in a background thread allows the server to:\n",
    "\n",
    "- Continuously listen for incoming requests from other agents (like the Root Agent)  \n",
    "- Keep the Jupyter notebook responsive for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71119a-a558-4042-9d8d-598d2ddb6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.server import A2AServer\n",
    "from utils.a2a_types import AgentCard, AgentCapabilities, AgentSkill, AgentAuthentication\n",
    "from utils.push_notification_auth import PushNotificationSenderAuth\n",
    "from utils.task_manager import AgentTaskManager\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "AUTH_USERNAME=\"burgeruser123\"\n",
    "AUTH_PASSWORD=\"burgerpass123\"\n",
    "\n",
    "def main(host, port):   \n",
    "\n",
    "    \"\"\"Starts the Burger Seller Agent server.\"\"\"\n",
    "    try:\n",
    "        capabilities = AgentCapabilities(pushNotifications=True)\n",
    "        skill = AgentSkill(\n",
    "            id=\"create_burger_order\",\n",
    "            name=\"Burger Order Creation Tool\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            tags=[\"burger order creation\"],\n",
    "            examples=[\"I want to order 2 classic cheeseburgers\"],\n",
    "        )\n",
    "        agent_card = AgentCard(\n",
    "            name=\"burger_seller_agent\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            # The URL provided here is for the sake of demo,\n",
    "            # in production you should use a proper domain name\n",
    "            url=f\"http://{host}:{port}/\",\n",
    "            version=\"1.0.0\",\n",
    "            authentication=AgentAuthentication(schemes=[\"Basic\"]),\n",
    "            defaultInputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            defaultOutputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            capabilities=capabilities,\n",
    "            skills=[skill],\n",
    "        )\n",
    "\n",
    "\n",
    "        notification_sender_auth = PushNotificationSenderAuth()\n",
    "        notification_sender_auth.generate_jwk()\n",
    "        server = A2AServer(\n",
    "            agent_card=agent_card,\n",
    "            task_manager=AgentTaskManager(\n",
    "                agent=BurgerSellerAgent(),\n",
    "                notification_sender_auth=notification_sender_auth,\n",
    "            ),\n",
    "            host=host,\n",
    "            port=port,\n",
    "            auth_username=AUTH_USERNAME,\n",
    "            auth_password=AUTH_PASSWORD,\n",
    "        )\n",
    "\n",
    "        server.app.add_route(\n",
    "            \"/.well-known/jwks.json\",\n",
    "            notification_sender_auth.handle_jwks_endpoint,\n",
    "            methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting server on {host}:{port}\")\n",
    "        server.start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during server startup: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11693ea",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "- We define the host (`0.0.0.0`) and a unique port (`10001`) for the Burger Agent. Once started, this agent is ready to respond to A2A requests from the Purchasing Agent.\n",
    "\n",
    "#### Why use a thread?  \n",
    "Running the server in a background thread means:\n",
    "- You can keep interacting with the notebook\n",
    "- Other agents or client components can still be started in additional cells\n",
    "\n",
    "This is essential for working with **multi-agent systems** in an interactive environment like Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e7532-cc6e-4f83-a457-d1cdb7214342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Global variable to hold the server thread reference ---\n",
    "# This allows you to stop it later from another cell if needed\n",
    "global server_thread\n",
    "server_thread = None\n",
    "\n",
    "# --- Main execution in the Jupyter cell ---\n",
    "if server_thread is not None and server_thread.is_alive():\n",
    "    print(\"Server is already running.\")\n",
    "else:\n",
    "    # Define host and port\n",
    "    server_host = \"0.0.0.0\"\n",
    "    server_port = 10001\n",
    "\n",
    "    # Create and start the thread\n",
    "    server_thread = threading.Thread(target=main, args=(server_host, server_port))\n",
    "    server_thread.daemon = True # Allows the main program to exit even if the thread is still running\n",
    "    server_thread.start()\n",
    "\n",
    "    print(f\"Server thread started. Waiting a moment for server to initialize on http://{server_host}:{server_port}\")\n",
    "    time.sleep(5) # Give it a few seconds to boot up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c790e",
   "metadata": {},
   "source": [
    "### Step 6: Test the Burger Seller Agent\n",
    "\n",
    "Now we can send a sample request to the Burger Agent endpoint and check that:\n",
    "\n",
    "- A well-structured response is returned  \n",
    "- The agent offers burger deals\n",
    "\n",
    "This confirms the agent is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a130f4d-ce41-4aae-9022-20bba37460e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BurgerSellerAgent()\n",
    "print(agent) \n",
    "result = agent.invoke(\"1 classic cheeseburger pls\", \"default_session\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49046124",
   "metadata": {},
   "source": [
    "# Part 3: Building the second Agent (Pizza Seller) with LangGraph and Ollama\n",
    "\n",
    "This agent is built with **LangGraph**, and its core **LLM** is served via **Ollama**. . It presents the pizza menu, provides pricing, and handles order creation through the A2A protocol. LangGraph creates the agent as a ReAct agent graph, implementing a cyclical pattern of Reasoning, Acting, and Observing\n",
    "\n",
    "In LangGraph, the ReAct pattern enables \n",
    "- **Reason**: The agent analyzes customer requests and menu options\n",
    "- **Act**: Execute actions by calling appropriate tools\n",
    "- **Observe**: Process the results to provide structured responses.\n",
    "\n",
    "Also, in this example we’ll create our custom tool, `create_pizza_order`, to handle deterministic order creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5ec63-8a7d-4eb1-81b4-341fd068bfeb",
   "metadata": {},
   "source": [
    "### Step 1: Serve a model with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596442a-8ac0-4853-9b1d-6e914859b980",
   "metadata": {},
   "source": [
    "To do this, you will need to download and install Ollama. You can do that, in a separate terminal, with a simple command: `curl -fsSL https://ollama.com/install.sh | sh` and after that, simply execute:  \n",
    "`ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e48e15",
   "metadata": {},
   "source": [
    "### Step 2: Define a tool (`create_pizza_order`)\n",
    "\n",
    "LLMs are great at understanding user intent and figuring out what actions to take, but as they don’t actually execute those actions themselves we use tools.\n",
    "\n",
    "The `create_pizza_order` tool takes a list of pizza items the user wants, generates a unique order ID, and builds a structured Order object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bec9e-2630-447b-b907-e5cc5d23a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    status: Literal[\"input_required\", \"completed\", \"error\"] = \"input_required\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: int\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    order_items: list[OrderItem]\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_pizza_order(order_items: list[OrderItem]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new pizza order with the given order items.\n",
    "\n",
    "    Args:\n",
    "        order_items: List of order items to be added to the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the order has been created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        order_id = str(uuid.uuid4())\n",
    "        order = Order(order_id=order_id, status=\"created\", order_items=order_items)\n",
    "        print(\"===\")\n",
    "        print(f\"order created: {order}\")\n",
    "        print(\"===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating order: {e}\")\n",
    "        return f\"Error creating order: {e}\"\n",
    "    return f\"Order {order.model_dump()} has been created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b7137",
   "metadata": {},
   "source": [
    "### Step 3: Define PizzaSellerAgent class  \n",
    "\n",
    "As mentioned before LangGraph creates the agent as a ReAct agent graph.\n",
    "\n",
    "This PizzaSellerAgent class is a wrapper around a ReAct graph and it constructs the Agent.\n",
    "\n",
    "The class PizzaSellerAgent defines the core intelligence and business rules of the pizza store agent. It:\n",
    "\n",
    "**- Encapsulates** the instructions, context, and rules the LLM must follow (menu, pricing, confirmation flow, error handling).  \n",
    "**- Connects** the LLM (via LangChain) with the create_pizza_order tool so that order creation is deterministic and safe.  \n",
    "**- Produces** a structured response format (ResponseFormat) that downstream systems (like the A2A server) can reliably consume.  \n",
    "**- Handles** different states (input_required, error, completed) and translates them into a uniform response for the server.  \n",
    "**- Acts** as the bridge between user queries and the A2A server, so that the server doesn’t need to know the conversation logic—it just hosts the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PizzaSellerAgent:\n",
    "    SYSTEM_INSTRUCTION = \"\"\"\n",
    "# INSTRUCTIONS\n",
    "\n",
    "You are a specialized assistant for a pizza store.\n",
    "Your sole purpose is to answer questions about what is available on pizza menu and price also handle order creation.\n",
    "If the user asks about anything other than pizza menu or order creation, politely state that you cannot help with that topic and can only assist with pizza menu and order creation.\n",
    "Do not attempt to answer unrelated questions or use tools for other purposes.\n",
    "\n",
    "# CONTEXT\n",
    "\n",
    "Provided below is the available pizza menu and it's related price:\n",
    "- Margherita Pizza: IDR 100K\n",
    "- Pepperoni Pizza: IDR 140K\n",
    "- Hawaiian Pizza: IDR 110K\n",
    "- Veggie Pizza: IDR 100K\n",
    "- BBQ Chicken Pizza: IDR 130K\n",
    "\n",
    "# RULES\n",
    "\n",
    "- If user want to do something, you will be following this order:\n",
    "    1. Always ensure the user already confirmed the order and total price. This confirmation may already given in the user query.\n",
    "    2. Use `create_pizza_order` tool to create the order\n",
    "    3. Finally, always provide response to the user about the detailed ordered items, price breakdown and total, and order ID\n",
    "\n",
    "- Set response status to input_required if asking for user order confirmation.\n",
    "- Set response status to error if there is an error while processing the request.\n",
    "- Set response status to completed if the request is complete.\n",
    "- DO NOT make up menu or price, Always rely on the provided menu given to you as context.\n",
    "\"\"\"\n",
    "    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"]\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = ChatOllama(\n",
    "            model=\"llama3.1:latest\" \n",
    "        )\n",
    "\n",
    "        self.tools = [create_pizza_order]\n",
    "        self.graph = create_react_agent(\n",
    "            self.model,\n",
    "            tools=self.tools,\n",
    "            checkpointer=memory,\n",
    "            prompt=self.SYSTEM_INSTRUCTION,\n",
    "            response_format=ResponseFormat,\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, sessionId) -> str:\n",
    "        config = {\"configurable\": {\"thread_id\": sessionId}}\n",
    "        self.graph.invoke({\"messages\": [(\"user\", query)]}, config)\n",
    "        return self.get_agent_response(config)\n",
    "\n",
    "    def get_agent_response(self, config):\n",
    "        current_state = self.graph.get_state(config)\n",
    "        structured_response = current_state.values.get(\"structured_response\")\n",
    "        if structured_response and isinstance(structured_response, ResponseFormat):\n",
    "            if structured_response.status == \"input_required\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "            elif structured_response.status == \"error\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "            elif structured_response.status == \"completed\":\n",
    "                return {\n",
    "                    \"is_task_complete\": True,\n",
    "                    \"require_user_input\": False,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"is_task_complete\": False,\n",
    "            \"require_user_input\": True,\n",
    "            \"content\": \"We are unable to process your request at the moment. Please try again.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e490f",
   "metadata": {},
   "source": [
    "### Step 4: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set these to the correct values for your setup\n",
    "os.environ[\"API_KEY\"] = \"pizza123\"\n",
    "os.environ[\"OLLAMA_MODEL\"] = \"ollama_chat/llama3.1:latest\"\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ac5e0",
   "metadata": {},
   "source": [
    "###  Step 5: Start the Pizza Agent Server\n",
    "\n",
    "We now launch the **Pizza Agent as an A2A server.**\n",
    "This wraps the Agent graph inside the A2AServer, so the agent is exposed as a discoverable **A2A service**.\n",
    "\n",
    "Running it in a background thread allows the server to:\n",
    "\n",
    "- Continuously listen for incoming requests from other agents (like the Root Agent)\n",
    "\n",
    "- Keep the Jupyter notebook responsive for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.server import A2AServer\n",
    "from utils.a2a_types import AgentCard, AgentCapabilities, AgentSkill, AgentAuthentication\n",
    "from utils.push_notification_auth import PushNotificationSenderAuth\n",
    "from utils.task_manager import AgentTaskManager\n",
    "import click\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main(host, port):\n",
    "    \"\"\"Starts the Pizza Seller Agent server.\"\"\"\n",
    "    try:\n",
    "        capabilities = AgentCapabilities(pushNotifications=True)\n",
    "        skill = AgentSkill(\n",
    "            id=\"create_pizza_order\",\n",
    "            name=\"Pizza Order Creation Tool\",\n",
    "            description=\"Helps with creating pizza orders\",\n",
    "            tags=[\"pizza order creation\"],\n",
    "            examples=[\"I want to order 2 pepperoni pizzas\"],\n",
    "        )\n",
    "        agent_card = AgentCard(\n",
    "            name=\"pizza_seller_agent\",\n",
    "            description=\"Helps with creating pizza orders\",\n",
    "            # The URL provided here is for the sake of demo,\n",
    "            # in production you should use a proper domain name\n",
    "            url=f\"http://{host}:{port}/\",\n",
    "            version=\"1.0.0\",\n",
    "            authentication=AgentAuthentication(schemes=[\"Bearer\"]),\n",
    "            defaultInputModes=PizzaSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            defaultOutputModes=PizzaSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            capabilities=capabilities,\n",
    "            skills=[skill],\n",
    "        )\n",
    "\n",
    "        notification_sender_auth = PushNotificationSenderAuth()\n",
    "        notification_sender_auth.generate_jwk()\n",
    "        server = A2AServer(\n",
    "            agent_card=agent_card,\n",
    "            task_manager=AgentTaskManager(\n",
    "                agent=PizzaSellerAgent(),\n",
    "                notification_sender_auth=notification_sender_auth,\n",
    "            ),\n",
    "            host=host,\n",
    "            port=port,\n",
    "            api_key=os.environ.get(\"API_KEY\"),\n",
    "        )\n",
    "\n",
    "        server.app.add_route(\n",
    "            \"/.well-known/jwks.json\",\n",
    "            notification_sender_auth.handle_jwks_endpoint,\n",
    "            methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting server on {host}:{port}\")\n",
    "        server.start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during server startup: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c526a07",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "- We define the host (`0.0.0.0`) and a unique port (`10000`) for the Pizza Agent. Once started, this agent is ready to respond to A2A requests from the Purchasing Agent.\n",
    "\n",
    "#### Why use a thread?\n",
    "Running the server in a background thread means:\n",
    "- You can keep interacting with the notebook\n",
    "- Other agents or client components can still be started in additional cells\n",
    "\n",
    "This is essential for working with **multi-agent systems** in an interactive environment like Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18594cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global server_thread\n",
    "server_thread = None\n",
    "\n",
    "# --- Main execution in the Jupyter cell ---\n",
    "if server_thread is not None and server_thread.is_alive():\n",
    "    print(\"Server is already running.\")\n",
    "else:\n",
    "    # Define host and port\n",
    "    server_host = \"0.0.0.0\"\n",
    "    server_port = 10000\n",
    "\n",
    "    # Create and start the thread\n",
    "    server_thread = threading.Thread(target=main, args=(server_host, server_port))\n",
    "    server_thread.daemon = True # Allows the main program to exit even if the thread is still running\n",
    "    server_thread.start()\n",
    "\n",
    "    print(f\"Server thread started. Waiting a moment for server to initialize on http://{server_host}:{server_port}\")\n",
    "    time.sleep(5) # Give it a few seconds to boot up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec95a8",
   "metadata": {},
   "source": [
    "### Step 6: Test the Pizza Seller Agent\n",
    "\n",
    "Now we can send a sample request to the Pizza Agent endpoint and check that:\n",
    "\n",
    "- A well-structured response is returned\n",
    "- The agent offers pizza deals\n",
    "\n",
    "This confirms the agent is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PizzaSellerAgent()\n",
    "print(agent) \n",
    "result = agent.invoke(\"I want to order 2 pepperoni pizzas\", \"default_session\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d86ea4",
   "metadata": {},
   "source": [
    "# Part 4: Building the last Agent (Purchasing Concierge) with Google ADK and Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36892a2a",
   "metadata": {},
   "source": [
    "This agent is built with **Google ADK** and it's core LLM is served via **Ollama**, It coordinates orders by delegating tasks to these seller agents through the open A2A protocol.  \n",
    "As we create the agent in ADK without using Google or Gemini models, we use LiteLLM to create the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf18405",
   "metadata": {},
   "source": [
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OLLAMA_MODEL\"] = \"ollama_chat/llama3.1:latest\"\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "os.environ[\"PIZZA_SELLER_AGENT_AUTH\"] = \"pizza123\"\n",
    "os.environ[\"PIZZA_SELLER_AGENT_URL\"] = \"http://localhost:10000\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_AUTH\"] = \"burgeruser123:burgerpass123\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_URL\"] = \"http://localhost:10001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb99be",
   "metadata": {},
   "source": [
    "### Step 2: Remote Agent Connections\n",
    "\n",
    "The root agent will delegate tasks to remote agents through the A2A protocol.\n",
    "\n",
    "The RemoteAgentConnections class is a communication wrapper that manages interactions with remote agents in the A2A ecosystem. It\n",
    "\n",
    "- **Establishes** A2A client connections with agent-specific authentication (API keys, basic auth).\n",
    "- **Sends** tasks to remote agents via send_task() method with callback support for real-time updates.\n",
    "- **Manages** metadata propagation and message ID tracking to maintain conversation continuity.\n",
    "- **Handles** task lifecycle events (status updates, artifact changes) through callback functions.\n",
    "- **Maintains** session state with conversation context and pending task tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03826e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import uuid\n",
    "from utils.a2a_types import (\n",
    "    AgentCard,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TaskStatusUpdateEvent,\n",
    "    TaskArtifactUpdateEvent,\n",
    ")\n",
    "from utils.client import A2AClient\n",
    "import os\n",
    "\n",
    "TaskCallbackArg = Task | TaskStatusUpdateEvent | TaskArtifactUpdateEvent\n",
    "TaskUpdateCallback = Callable[[TaskCallbackArg, AgentCard], Task]\n",
    "\n",
    "KNOWN_AUTH = {\n",
    "    \"pizza_seller_agent\": os.getenv(\"PIZZA_SELLER_AGENT_AUTH\", \"api_key\"),\n",
    "    \"burger_seller_agent\": os.getenv(\"BURGER_SELLER_AGENT_AUTH\", \"user:pass\"),\n",
    "}\n",
    "\n",
    "\n",
    "class RemoteAgentConnections:\n",
    "    \"\"\"A class to hold the connections to the remote agents.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_card: AgentCard, agent_url: str):\n",
    "        auth = KNOWN_AUTH.get(agent_card.name, None)\n",
    "        self.agent_client = A2AClient(agent_card, auth=auth, agent_url=agent_url)\n",
    "        self.card = agent_card\n",
    "\n",
    "        self.conversation_name = None\n",
    "        self.conversation = None\n",
    "        self.pending_tasks = set()\n",
    "\n",
    "    def get_agent(self) -> AgentCard:\n",
    "        return self.card\n",
    "\n",
    "    async def send_task(\n",
    "        self,\n",
    "        request: TaskSendParams,\n",
    "        task_callback: TaskUpdateCallback | None,\n",
    "    ) -> Task | None:\n",
    "        response = await self.agent_client.send_task(request.model_dump())\n",
    "        merge_metadata(response.result, request)\n",
    "        # For task status updates, we need to propagate metadata and provide\n",
    "        # a unique message id.\n",
    "        if (\n",
    "            hasattr(response.result, \"status\")\n",
    "            and hasattr(response.result.status, \"message\")\n",
    "            and response.result.status.message\n",
    "        ):\n",
    "            merge_metadata(response.result.status.message, request.message)\n",
    "            m = response.result.status.message\n",
    "            if not m.metadata:\n",
    "                m.metadata = {}\n",
    "            if \"message_id\" in m.metadata:\n",
    "                m.metadata[\"last_message_id\"] = m.metadata[\"message_id\"]\n",
    "            m.metadata[\"message_id\"] = str(uuid.uuid4())\n",
    "\n",
    "        if task_callback:\n",
    "            task_callback(response.result, self.card)\n",
    "        return response.result\n",
    "\n",
    "\n",
    "def merge_metadata(target, source):\n",
    "    if not hasattr(target, \"metadata\") or not hasattr(source, \"metadata\"):\n",
    "        return\n",
    "    if target.metadata and source.metadata:\n",
    "        target.metadata.update(source.metadata)\n",
    "    elif source.metadata:\n",
    "        target.metadata = dict(**source.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf4245",
   "metadata": {},
   "source": [
    "### Step 3: Define PurchasingAgent class \n",
    "\n",
    "As mentioned ealier we create Agent with non-google or gemini model using LiteLLM. \n",
    "\n",
    "Now this PurchasingAgent class is a wrapper around AgentCard and Agent components.\n",
    "\n",
    "The class PurchasingAgent defines the core orchestration and delegation logic. It:\n",
    "- **Discovers** remote agents by resolving Agent Cards, establishing authenticated connections, and maintaining a seller agent registry.\n",
    "- **Orchestrates** task delegation through intelligent routing, session management, and asynchronous task handling\n",
    "- **Manage** responses by aggregating seller outputs, formatting user messages, and handling state transitions.\n",
    "- **Controls** conversation flow by tracking agent assignments, managing metadata propagation, and coordinating multi-agent interactions.\n",
    "- **Provides** reliability through error handling, clean session management, and structured response validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "from google.adk.models.lite_llm import LiteLlm \n",
    "from google.adk import Agent\n",
    "from google.adk.agents.readonly_context import ReadonlyContext\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from utils.card_resolver import A2ACardResolver\n",
    "from utils.a2a_types import (\n",
    "    AgentCard,\n",
    "    Message,\n",
    "    TaskState,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TextPart,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "\n",
    "class PurchasingAgent:\n",
    "    \"\"\"The purchasing agent.\n",
    "\n",
    "    This is the agent responsible for choosing which remote seller agents to send\n",
    "    tasks to and coordinate their work.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remote_agent_addresses: List[str],\n",
    "        task_callback: TaskUpdateCallback | None = None,\n",
    "    ):\n",
    "        self.task_callback = task_callback\n",
    "        self.remote_agent_connections: dict[str, RemoteAgentConnections] = {}\n",
    "        self.cards: dict[str, AgentCard] = {}\n",
    "        for address in remote_agent_addresses:\n",
    "            card_resolver = A2ACardResolver(address)\n",
    "            try:\n",
    "                card = card_resolver.get_agent_card()\n",
    "                # The URL accessed here should be the same as the one provided in the agent card\n",
    "                # However, in this demo we are using the URL provided in the key arguments\n",
    "                remote_connection = RemoteAgentConnections(\n",
    "                    agent_card=card, agent_url=address\n",
    "                )\n",
    "                self.remote_agent_connections[card.name] = remote_connection\n",
    "                self.cards[card.name] = card\n",
    "            except httpx.ConnectError:\n",
    "                print(f\"ERROR: Failed to get agent card from : {address}\")\n",
    "        agent_info = []\n",
    "        for ra in self.list_remote_agents():\n",
    "            agent_info.append(json.dumps(ra))\n",
    "        self.agents = \"\\n\".join(agent_info)\n",
    "\n",
    "    def create_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            model=LiteLlm(model=os.getenv(\"OLLAMA_MODEL\")), \n",
    "            name=\"purchasing_agent\",\n",
    "            instruction=self.root_instruction,\n",
    "            before_model_callback=self.before_model_callback,\n",
    "            description=(\n",
    "                \"This purchasing agent orchestrates the decomposition of the user purchase request into\"\n",
    "                \" tasks that can be performed by the seller agents.\"\n",
    "            ),\n",
    "            tools=[\n",
    "                self.send_task,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def root_instruction(self, context: ReadonlyContext) -> str:\n",
    "        current_agent = self.check_active_agent(context)\n",
    "        return f\"\"\"You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
    "appropriate seller remote agents.\n",
    "\n",
    "Execution:\n",
    "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
    "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
    "    So improve the task description to include all the necessary information related to that agent\n",
    "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
    "    connect with them without asking user permission or asking user preference\n",
    "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
    "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
    "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
    "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
    "- Never ask order confirmation to the remote seller agent \n",
    "\n",
    "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
    "Focus on the most recent parts of the conversation primarily.\n",
    "\n",
    "If there is an active agent, send the request to that agent with the update task tool.\n",
    "\n",
    "Agents:\n",
    "{self.agents}\n",
    "\n",
    "Current active seller agent: {current_agent[\"active_agent\"]}\n",
    "\"\"\"\n",
    "\n",
    "    def check_active_agent(self, context: ReadonlyContext):\n",
    "        state = context.state\n",
    "        if (\n",
    "            \"session_id\" in state\n",
    "            and \"session_active\" in state\n",
    "            and state[\"session_active\"]\n",
    "            and \"active_agent\" in state\n",
    "        ):\n",
    "            return {\"active_agent\": f\"{state['active_agent']}\"}\n",
    "        return {\"active_agent\": \"None\"}\n",
    "\n",
    "    def before_model_callback(self, callback_context: CallbackContext, llm_request):\n",
    "        state = callback_context.state\n",
    "        if \"session_active\" not in state or not state[\"session_active\"]:\n",
    "            if \"session_id\" not in state:\n",
    "                state[\"session_id\"] = str(uuid.uuid4())\n",
    "            state[\"session_active\"] = True\n",
    "\n",
    "    def list_remote_agents(self):\n",
    "        \"\"\"List the available remote agents you can use to delegate the task.\"\"\"\n",
    "        if not self.remote_agent_connections:\n",
    "            return []\n",
    "\n",
    "        remote_agent_info = []\n",
    "        for card in self.cards.values():\n",
    "            print(f\"Found agent card: {card.model_dump()}\")\n",
    "            print(\"=\" * 100)\n",
    "            remote_agent_info.append(\n",
    "                {\"name\": card.name, \"description\": card.description}\n",
    "            )\n",
    "        return remote_agent_info\n",
    "\n",
    "    async def send_task(self, agent_name: str, task: str, tool_context: ToolContext):\n",
    "        \"\"\"Sends a task to remote seller agent\n",
    "\n",
    "        This will send a message to the remote agent named agent_name.\n",
    "\n",
    "        Args:\n",
    "            agent_name: The name of the agent to send the task to.\n",
    "            task: The comprehensive conversation context summary\n",
    "                and goal to be achieved regarding user inquiry and purchase request.\n",
    "            tool_context: The tool context this method runs in.\n",
    "\n",
    "        Yields:\n",
    "            A dictionary of JSON data.\n",
    "        \"\"\"\n",
    "        if agent_name not in self.remote_agent_connections:\n",
    "            raise ValueError(f\"Agent {agent_name} not found\")\n",
    "        state = tool_context.state\n",
    "        state[\"active_agent\"] = agent_name\n",
    "        client = self.remote_agent_connections[agent_name]\n",
    "        if not client:\n",
    "            raise ValueError(f\"Client not available for {agent_name}\")\n",
    "        if \"task_id\" in state:\n",
    "            taskId = state[\"task_id\"]\n",
    "        else:\n",
    "            taskId = str(uuid.uuid4())\n",
    "        sessionId = state[\"session_id\"]\n",
    "        task: Task\n",
    "        messageId = \"\"\n",
    "        metadata = {}\n",
    "        if \"input_message_metadata\" in state:\n",
    "            metadata.update(**state[\"input_message_metadata\"])\n",
    "            if \"message_id\" in state[\"input_message_metadata\"]:\n",
    "                messageId = state[\"input_message_metadata\"][\"message_id\"]\n",
    "        if not messageId:\n",
    "            messageId = str(uuid.uuid4())\n",
    "        metadata.update(**{\"conversation_id\": sessionId, \"message_id\": messageId})\n",
    "        request: TaskSendParams = TaskSendParams(\n",
    "            id=taskId,\n",
    "            sessionId=sessionId,\n",
    "            message=Message(\n",
    "                role=\"user\",\n",
    "                parts=[TextPart(text=task)],\n",
    "                metadata=metadata,\n",
    "            ),\n",
    "            acceptedOutputModes=[\"text\", \"text/plain\"],\n",
    "            # pushNotification=None,\n",
    "            metadata={\"conversation_id\": sessionId},\n",
    "        )\n",
    "        task = await client.send_task(request, self.task_callback)\n",
    "        # Assume completion unless a state returns that isn't complete\n",
    "        state[\"session_active\"] = task.status.state not in [\n",
    "            TaskState.COMPLETED,\n",
    "            TaskState.CANCELED,\n",
    "            TaskState.FAILED,\n",
    "            TaskState.UNKNOWN,\n",
    "        ]\n",
    "        if task.status.state == TaskState.INPUT_REQUIRED:\n",
    "            # Force user input back\n",
    "            tool_context.actions.escalate = True\n",
    "        elif task.status.state == TaskState.COMPLETED:\n",
    "            # Reset active agent is task is completed\n",
    "            state[\"active_agent\"] = \"None\"\n",
    "\n",
    "        response = []\n",
    "        if task.status.message:\n",
    "            # Assume the information is in the task message.\n",
    "            response.extend(convert_parts(task.status.message.parts, tool_context))\n",
    "        if task.artifacts:\n",
    "            for artifact in task.artifacts:\n",
    "                response.extend(convert_parts(artifact.parts, tool_context))\n",
    "        return response\n",
    "\n",
    "\n",
    "def convert_parts(parts: list[Part], tool_context: ToolContext):\n",
    "    rval = []\n",
    "    for p in parts:\n",
    "        rval.append(convert_part(p, tool_context))\n",
    "    return rval\n",
    "\n",
    "\n",
    "def convert_part(part: Part, tool_context: ToolContext):\n",
    "    # Currently only support text parts\n",
    "    if part.type == \"text\":\n",
    "        return part.text\n",
    "\n",
    "    return f\"Unknown type: {part.type}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_agent = PurchasingAgent(\n",
    "    remote_agent_addresses=[\n",
    "        os.getenv(\"PIZZA_SELLER_AGENT_URL\", \"http://localhost:10000\"),\n",
    "        os.getenv(\"BURGER_SELLER_AGENT_URL\", \"http://localhost:10001\"),\n",
    "    ]\n",
    ").create_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a46ebd",
   "metadata": {},
   "source": [
    "# Part5: Running the Purchasing Concierge Agent with a UI\n",
    "\n",
    "To make it easier to test the agent, we’ve built a **simple Gradio interface**.\n",
    "\n",
    "- It runs in your browser at: **http://localhost:8084** (local)\n",
    "- #### Try Asking for a Menu!\n",
    "\n",
    "    Once all the agents are running, you can test their responses by asking:\n",
    "\n",
    "    > “Can I see the menu?”  \n",
    "\n",
    "    > or  \n",
    "\n",
    "    > “What pizza options do you have today?” / “What burgers are on the menu?”\n",
    "\n",
    "    Each seller agent (Burger & Pizza) will reply with their available items and prices.  \n",
    "    This is a great way to test if the agents are reachable and responding properly.\n",
    "\n",
    "\n",
    "\n",
    "This is helpful for testing how the full multi-agent system works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944436d-b091-43c1-bab5-98662e68e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f31045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict, Any\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.events import Event\n",
    "from typing import AsyncIterator\n",
    "from google.genai import types\n",
    "from pprint import pformat\n",
    "import logging\n",
    "logging.getLogger(\"opentelemetry.context\").setLevel(logging.CRITICAL)\n",
    "\n",
    "purchasing_agent = root_agent\n",
    "\n",
    "APP_NAME = \"purchasing_concierge_app\"\n",
    "USER_ID = \"default_user\"\n",
    "SESSION_ID = \"default_session\"\n",
    "SESSION_SERVICE = InMemorySessionService()\n",
    "\n",
    "PURCHASING_AGENT_RUNNER = Runner(\n",
    "    agent=purchasing_agent,  # The agent we want to run\n",
    "    app_name=APP_NAME,  # Associates runs with our app\n",
    "    session_service=SESSION_SERVICE,  # Uses our session manager\n",
    ")\n",
    "\n",
    "async def get_response_from_agent(\n",
    "    message: str,\n",
    "    history: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    \"\"\"Send the message to the backend and get a response.\n",
    "\n",
    "    Args:\n",
    "        message: Text content of the message.\n",
    "        history: List of previous message dictionaries in the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Text response from the backend service.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    events_iterator: AsyncIterator[Event] = PURCHASING_AGENT_RUNNER.run_async(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        new_message=types.Content(role=\"user\", parts=[types.Part(text=message)]),\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    async for event in events_iterator:  # event has type Event\n",
    "        if event.content.parts:\n",
    "            for part in event.content.parts:\n",
    "                if part.function_call:\n",
    "                    formatted_call = f\"```python\\n{pformat(part.function_call.model_dump(), indent=2, width=80)}\\n```\"\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=f\"{part.function_call.name}:\\n{formatted_call}\",\n",
    "                            metadata={\"title\": \"🛠️ Tool Call\"},\n",
    "                        )\n",
    "                    )\n",
    "                elif part.function_response:\n",
    "                    formatted_response = f\"```python\\n{pformat(part.function_response.model_dump(), indent=2, width=80)}\\n```\"\n",
    "\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=formatted_response,\n",
    "                            metadata={\"title\": \"⚡ Tool Response\"},\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Key Concept: is_final_response() marks the concluding message for the turn\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                # Extract text from the first part\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            elif event.actions and event.actions.escalate:\n",
    "                # Handle potential errors/escalations\n",
    "                final_response_text = (\n",
    "                    f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "                )\n",
    "            responses.append(\n",
    "                gr.ChatMessage(role=\"assistant\", content=final_response_text)\n",
    "            )\n",
    "            yield responses\n",
    "            break  # Stop processing events once the final response is found\n",
    "\n",
    "        yield responses\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    session = await SESSION_SERVICE.create_session(\n",
    "    app_name=APP_NAME, \n",
    "    user_id=USER_ID, \n",
    "    session_id=SESSION_ID)\n",
    "\n",
    "    print(session)\n",
    "    demo = gr.ChatInterface(\n",
    "        get_response_from_agent,\n",
    "        title=\"Purchasing Concierge\",\n",
    "        description=\"This assistant can help you to purchase food from remote sellers.\",\n",
    "        type=\"messages\",\n",
    "    )\n",
    "    print(SESSION_ID)\n",
    "\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=8095,\n",
    "        share=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amd_workshop] *",
   "language": "python",
   "name": "conda-env-amd_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
