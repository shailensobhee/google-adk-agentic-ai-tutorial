{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af912e0",
   "metadata": {},
   "source": [
    "## <b> Google ADK + AMD Instinct™ GPUs: The Dynamic Duo for AI Agents </b>\n",
    "\n",
    "This notebook walks through building a real-world Agent-to-Agent (A2A) system for a Purchasing Concierge, where multiple specialized agents (Root, Burger, Pizza) work together seamlessly.\n",
    "\n",
    "**Root Agent (Google ADK)** – orchestrates conversations and routes user requests, using LiteLlm with a locally hosted Ollama model.  \n",
    "**Burger Seller Agent (CrewAI + vLLM)** – presents the burger menu, provides pricing, and handles order creation, powered by a vLLM-hosted model.  \n",
    "**Pizza Seller Agent (LangGraph + Ollama)** – specializes in pizza ordering, built with LangGraph on top of Ollama.\n",
    "\n",
    "The root agent coordinates orders by delegating tasks to these seller agents (Pizza and Burger) through the open A2A protocol, enabling seamless collaboration across frameworks.\n",
    "\n",
    "The system runs locally on AMD GPUs and includes an interactive Gradio UI, showcasing real-world agent interoperability and cross-framework integration.\n",
    "\n",
    "<img src=\"./assets/image.png\" alt=\"agents-architecture\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f431a2-cd3b-407d-afa2-71b576db1d1e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Use the following setup to run this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be695411-785b-4a38-a0bd-b694c7c515b0",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "\n",
    "For this tutorial, you'll need a system with an AMD Instinct GPU. To run the model on the CPU and use AMD ZenDNN, you need an AMD EPYC CPU. \n",
    "\n",
    "This tutorial was tested on the following hardware:\n",
    "* AMD Instinct MI100\n",
    "* AMD Instinct MI210\n",
    "* AMD Instinct MI300X\n",
    "* 4th generation AMD EPYC (Genoa)\n",
    "* 5th generation AMD EPYC (Turin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135ed61-bba4-4f0a-be93-dd7637517b0e",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04 or later.\n",
    "* **ROCm 6.3**: This is only required for GPU execution. Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "* **PyTorch 2.6** (or later)\n",
    "* **vLLM** serving a model of your choice in a Docker container (requires `sudo` rights)\n",
    "* **Ollama**: this will be used to serve another model of your choice\n",
    "  \n",
    "### Install and launch Jupyter Notebooks\n",
    "If Jupyter is not already installed on your system, install it and launch JupyterLab using the following commands:\n",
    "\n",
    "```\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "To start the Jupyter server, run the following command:\n",
    "\n",
    "```\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "After the command executes, the terminal output displays a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue to follow the steps in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273702c6",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "To run this notebook locally, you will first need a bunch of \"utilities\" which include some definition of the Google A2A types, logic on resolving Agent Cards, and a few other files on Task Management. These utility files enable third party agentic frameworks that we use in this project (ex: CrewAI and LangGraph) to be compatible with the Google A2A protocol. \n",
    "\n",
    "We first download the utlities and move the cloned files one folder level above so that we can easily use them throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40fd3a7f-c31a-4609-98a8-14c2a74e5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning the repository for the first time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'google-adk-agentic-ai-tutorial'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "REPO_URL=\"https://github.com/shailensobhee/google-adk-agentic-ai-tutorial.git\"\n",
    "BRANCH_NAME=\"utils\"\n",
    "REPO_FOLDER=\"google-adk-agentic-ai-tutorial\"\n",
    "\n",
    "# Check if the repository folder already exists\n",
    "if [ -d \"$REPO_FOLDER\" ]; then\n",
    "  echo \"The repository already exists. Updating it...\"\n",
    "  cd \"$REPO_FOLDER\"\n",
    "  git pull origin \"$BRANCH_NAME\"\n",
    "  cd ..\n",
    "else\n",
    "  echo \"Cloning the repository for the first time...\"\n",
    "  git clone --single-branch --branch \"$BRANCH_NAME\" \"$REPO_URL\"\n",
    "fi\n",
    "cp -r $REPO_FOLDER/* .\n",
    "rm -rf $REPO_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872929e-6dc0-4009-a87c-0b68387af998",
   "metadata": {},
   "source": [
    "We then install all required Python packages that we will need throughout the rest of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d6311fe-7171-4847-9508-e81168456a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crewai in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 1)) (0.152.0)\n",
      "Requirement already satisfied: langchain_ollama in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: langgraph in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 3)) (0.6.6)\n",
      "Requirement already satisfied: google-adk>=0.3.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 4)) (1.13.0)\n",
      "Requirement already satisfied: gradio>=5.28.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 5)) (5.44.1)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jwcrypto>=1.5.6 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 7)) (1.5.6)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 8)) (2.11.7)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 9)) (2.10.1)\n",
      "Requirement already satisfied: sse-starlette>=2.3.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 10)) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.46.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 11)) (0.47.3)\n",
      "Requirement already satisfied: typing-extensions>=4.13.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 12)) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.34.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 13)) (0.35.0)\n",
      "Requirement already satisfied: litellm==1.74.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 14)) (1.74.3)\n",
      "Requirement already satisfied: openai==1.99.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 15)) (1.99.1)\n",
      "Requirement already satisfied: yfinance in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 16)) (0.2.65)\n",
      "Requirement already satisfied: dotenv in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from -r requirements/requirements.txt (line 17)) (0.9.9)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (3.12.15)\n",
      "Requirement already satisfied: click in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (8.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (8.7.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (4.25.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (1.1.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (0.11.0)\n",
      "Requirement already satisfied: tokenizers in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from litellm==1.74.3->-r requirements/requirements.txt (line 14)) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openai==1.99.1->-r requirements/requirements.txt (line 15)) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openai==1.99.1->-r requirements/requirements.txt (line 15)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openai==1.99.1->-r requirements/requirements.txt (line 15)) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openai==1.99.1->-r requirements/requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openai==1.99.1->-r requirements/requirements.txt (line 15)) (4.67.1)\n",
      "Requirement already satisfied: certifi in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from httpx>=0.28.1->-r requirements/requirements.txt (line 6)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from httpx>=0.28.1->-r requirements/requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from httpx>=0.28.1->-r requirements/requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pydantic>=2.10.6->-r requirements/requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pydantic>=2.10.6->-r requirements/requirements.txt (line 8)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pydantic>=2.10.6->-r requirements/requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.28.1->-r requirements/requirements.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (0.27.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: chromadb>=0.5.23 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.0.20)\n",
      "Requirement already satisfied: instructor>=1.3.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: json-repair==0.25.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (0.25.2)\n",
      "Requirement already satisfied: json5>=0.10.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: jsonref>=1.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: onnxruntime==1.22.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: openpyxl>=3.1.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.30.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.30.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: pdfplumber>=0.11.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (0.11.7)\n",
      "Requirement already satisfied: portalocker==2.7.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: pyvis>=0.3.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: tomli-w>=1.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: tomli>=2.0.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: uv>=0.4.25 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from crewai->-r requirements/requirements.txt (line 1)) (0.8.15)\n",
      "Requirement already satisfied: coloredlogs in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (2.3.2)\n",
      "Requirement already satisfied: packaging in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: protobuf in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (6.32.0)\n",
      "Requirement already satisfied: sympy in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain_ollama->-r requirements/requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain_ollama->-r requirements/requirements.txt (line 2)) (0.3.75)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (0.4.23)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph->-r requirements/requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph->-r requirements/requirements.txt (line 3)) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph->-r requirements/requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph->-r requirements/requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r requirements/requirements.txt (line 3)) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r requirements/requirements.txt (line 3)) (3.11.3)\n",
      "Requirement already satisfied: absolufy-imports<1.0.0,>=0.3.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.3.1)\n",
      "Requirement already satisfied: authlib<2.0.0,>=1.5.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: fastapi<1.0.0,>=0.115.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.116.1)\n",
      "Requirement already satisfied: google-api-python-client<3.0.0,>=2.157.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.181.0)\n",
      "Requirement already satisfied: google-cloud-bigtable>=2.32.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.32.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.95.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.111.0)\n",
      "Requirement already satisfied: google-cloud-secret-manager<3.0.0,>=2.22.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-spanner<4.0.0,>=3.56.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.57.0)\n",
      "Requirement already satisfied: google-cloud-speech<3.0.0,>=2.30.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.33.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.19.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.21.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.33.0)\n",
      "Requirement already satisfied: graphviz<1.0.0,>=0.20.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.21)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.8.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.13.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-gcp-trace<2.0.0,>=1.9.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0.post0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy-spanner>=1.14.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.0.43)\n",
      "Requirement already satisfied: tzlocal<6.0,>=5.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: websockets<16.0.0,>=15.0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: cryptography in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from authlib<2.0.0,>=1.5.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (45.0.7)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.30.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.40.3)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.36.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: docstring_parser<1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.17.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.74.0)\n",
      "Requirement already satisfied: cloudpickle<4.0,>=3.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: google-cloud-trace<2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.16.2)\n",
      "Requirement already satisfied: google-cloud-logging<4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.12.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.95.1->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.7.2)\n",
      "Requirement already satisfied: google-cloud-appengine-logging<2.0.0,>=0.1.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.6.2)\n",
      "Requirement already satisfied: google-cloud-audit-log<1.0.0,>=0.3.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-logging<4->google-cloud-aiplatform[agent-engines]<2.0.0,>=1.95.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.14.2)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-spanner<4.0.0,>=3.56.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: grpc-interceptor>=0.15.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-spanner<4.0.0,>=3.56.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.15.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.0.20)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-resourcedetector-gcp==1.*,>=1.5.0dev0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from opentelemetry-exporter-gcp-trace<2.0.0,>=1.9.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.9.0a0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from opentelemetry-sdk>=1.30.0->crewai->-r requirements/requirements.txt (line 1)) (0.57b0)\n",
      "Requirement already satisfied: six>=1.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.9.0.post0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.4->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3.0.0,>=2.157.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: greenlet>=1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from sqlalchemy<3.0.0,>=2.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (3.2.4)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (24.1.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: ffmpy in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.12.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (1.12.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.34.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (11.3.0)\n",
      "Requirement already satisfied: pydub in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.25.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.12.11)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2.10.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.17.3)\n",
      "Requirement already satisfied: fsspec in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from gradio-client==1.12.1->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2025.9.0)\n",
      "Requirement already satisfied: filelock in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (3.19.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (1.1.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (14.1.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (4.4.0)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (4.13.5)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from yfinance->-r requirements/requirements.txt (line 16)) (0.13.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from aiohttp>=3.10->litellm==1.74.3->-r requirements/requirements.txt (line 14)) (1.20.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirements/requirements.txt (line 16)) (2.8)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (5.4.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (6.5.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: pyproject_hooks in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from cryptography->authlib<2.0.0,>=1.5.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from cffi>=1.14->cryptography->authlib<2.0.0,>=1.5.1->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (2.22)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from instructor>=1.3.3->crewai->-r requirements/requirements.txt (line 1)) (5.6.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (0.10)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_ollama->-r requirements/requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.28.0->-r requirements/requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from openpyxl>=3.1.5->crewai->-r requirements/requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.36.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pdfplumber>=0.11.4->crewai->-r requirements/requirements.txt (line 1)) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pdfplumber>=0.11.4->crewai->-r requirements/requirements.txt (line 1)) (4.30.0)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (9.5.0)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (4.1.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: decorator in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: alembic in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from sqlalchemy-spanner>=1.14.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.16.5)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai->-r requirements/requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: Mako in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from alembic->sqlalchemy-spanner>=1.14.0->google-adk>=0.3.0->-r requirements/requirements.txt (line 4)) (1.3.10)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from coloredlogs->onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from stack_data->ipython>=5.3.0->pyvis>=0.3.2->crewai->-r requirements/requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/shailens/miniforge3/envs/lab_test/lib/python3.12/site-packages (from sympy->onnxruntime==1.22.0->crewai->-r requirements/requirements.txt (line 1)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd9cb6",
   "metadata": {},
   "source": [
    "###  <b> Getting Started with A2A </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c08e72",
   "metadata": {},
   "source": [
    "For **creating an A2A system**, the first thing we need is an **Agent**.  \n",
    "An agent is essentially a smart computer program designed to act on a human’s behalf, much like a personal assistant. The agents use **large language models (LLMs) as their \"brain\"**. It perceives inputs, reasons with them and acts to achieve goals.\n",
    "\n",
    "Once we have an agent, the next piece is an **Agent Card**.  \n",
    "This is a simple JSON metadata document that describes the agent’s identity, capabilities, and endpoint. This is more of the agent’s “business card”, it tells others who the agent is, what it can do, and how to interact with it. Agent Cards are the foundation of how agents discover and talk to each other in the A2A ecosystem.\n",
    "\n",
    "Next comes the **A2A Protocol**, which makes agents communicate and collaborate.  \n",
    "The A2A protocol is an open standard that provides a common language for agents built with different frameworks or by different vendors. It introduces two key roles:\n",
    "\n",
    "**A2A Client** – An application or agent that initiates a request on behalf of the user. In our project, the **Root Agent (ADK)** plays this role when it calls out to seller agents.  \n",
    "\n",
    "**A2A Server** – An agent or agentic system that exposes an HTTP endpoint to accept requests, process tasks, and return results. In our project, the **Burger Agent (CrewAI)** and **Pizza Agent (LangChain)** serve as A2A Servers.  \n",
    "\n",
    "These agents communicate by exchanging Tasks (a unit of work, e.g., ‘place a burger order’) made up of multiple Messages.\n",
    "\n",
    "Together, **Agents, Agent Cards, and the A2A Protocol (with Client/Server roles)** form the foundation for building interconnected agentic systems. With frameworks like the **Agent Development Kit (ADK)**, we can easily create our own agents and connect them into larger ecosystems where a Root Agent can discover, call, and coordinate remote agents running behind A2A servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb99a2",
   "metadata": {},
   "source": [
    "### **How AI Agents Keep Your Conversation Alive**\n",
    "\n",
    "Just like humans rely on memory to maintain coherent conversations, AI agents need a way to track context across multiple exchanges. Without this capability, each interaction would be isolated-imagine talking to someone who forgets everything you said the moment after you say it.\n",
    "\n",
    "The Agent Development Kit (ADK) solves this through three core components:\n",
    "- **Session** : Maintains the current conversation thread and keeps track of the ongoing conversation from start to finish, including every message exchanged and action taken during that specific chat.\n",
    "- **State** : Is your conversation's \"scratch pad\". It holds temporary data that's only relevant to your current conversation thread, like items you've added to a cart or preferences you've mentioned right now. This data gets cleared when the conversation ends.\n",
    "- **Memory** : Acts as a searchable knowledge base that spans across all your past sessions, letting the agent recall and reference information or context beyond the current conversation.\n",
    "\n",
    "These components work together to create agents that can reference previous conversations and build meaningful conversational context over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0efcc",
   "metadata": {},
   "source": [
    "### **Security Considerations in Multi-Agent System**\n",
    "\n",
    "The A2A Protocol is designed with enterprise standards as its core, ensuring agents can collaborate securely and at scale. Instead of new standards, it builds on widely adopted enterprise practices.\n",
    "\n",
    "- **Secure**: All communication happens over HTTPS with modern TLS, certificate validation, and protection against eavesdropping or tampering.\n",
    "- **Authentication**: Each agent server enforces authentication using standard web methods (OAuth2, OpenID Connect, API keys), with credentials passed in HTTP headers and managed outside the protocol.\n",
    "- **Authorization**: Enforces granular, skill-based, and least-privilege access controls, ensuring agents only perform permitted actions.\n",
    "- **Observability**: Integrates with tracing, logging, and monitoring tools to provide visibility, debugging, and auditing.\n",
    "- **API Management**: Agent endpoints are discoverable via Agent Cards, supporting centralized governance and easy onboarding of new agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c6c1c",
   "metadata": {},
   "source": [
    "### <b>Example Walkthrough </b>\n",
    "\n",
    "In this example we primarly focus on creating 2 remote agents (BurgerSellerAgent and PizzaSellerAgent) and 1 root agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f964d8",
   "metadata": {},
   "source": [
    "### **Remote Seller 1: Burger Agent**\n",
    "\n",
    "This agent is built with **CrewAI**, and its core **LLM** is served via **vLLM**. It presents the burger menu, provides pricing, and handles order creation through the A2A protocol. As we create this remote agent using CrewAI, the standard CrewAI agent structure is:\n",
    "\n",
    "***Agent → Task → Crew → Output***\n",
    "\n",
    "- **Agent**: The specialized entity (role, goal, backstory, tools, LLM).  \n",
    "- **Task**: The specific work the agent will perform, with detailed instructions and expected output format.  \n",
    "- **Crew**: The orchestrator that links agents and tasks together, defines process flow, and executes the workflow.  \n",
    "\n",
    "In CrewAI, tasks must always be linked to agents **through a Crew** in order to run.\n",
    "\n",
    "Additionally, in this example we’ll create a custom tool, `create_burger_order`, to handle deterministic order creation.\n",
    "\n",
    "Now, let’s implement the Burger agent step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2159ad-4889-457a-b5ad-c4787f44d192",
   "metadata": {},
   "source": [
    "### Step 1: Serve a vLLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e658d3-eda1-4d1b-ae37-145f37c4ef9e",
   "metadata": {},
   "source": [
    "To get started with serving a model of your choice with vLLM, you'll need to run a few commands in a separate terminal. The following steps will guide you through the process, which involves running a vLLM Docker container and serving a model with specific configurations for tool-calling agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964561f6-80ad-4a0e-b2c0-3137460da936",
   "metadata": {},
   "source": [
    "`docker run -d -p 8088:8088 -it --ipc=host --network=host --privileged --cap-add=CAP_SYS_ADMIN --device=/dev/kfd --device=/dev/dri --device=/dev/mem --group-add render --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -e HF_TOKEN=$HF_TOKEN --name vllm_server  rocm/vllm:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1df45b-1016-4c5a-9e38-9a0f0f121878",
   "metadata": {},
   "source": [
    "Then, attach to that container: `docker attach vllm_server`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60787223-a9dd-442a-aaf8-aadbbba1c0b6",
   "metadata": {},
   "source": [
    "#### Step1a: Serve the model with Tool-Calling capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec33f43-59bf-4d8f-89bc-8c475c46d447",
   "metadata": {},
   "source": [
    "Once inside the container, you can serve your model. The provided command uses a tool-call parser and a specific chat template, which are crucial for agentic AI applications that require structured function calls.\n",
    "\n",
    "Export your Hugging Face token to enable access to gated models.  \n",
    "`export HF_TOKEN=<your HF token>`  \n",
    "\n",
    "Run the vllm serve command to start the model. This example uses `meta-llama/Llama-3.1-8B-Instruct`, but you can replace it with any model you prefer.  \n",
    "`--enable-auto-tool-choice`: Activates the model's ability to automatically select the appropriate tool.  \n",
    "`--tool-call-parser llama3_json`: Specifies the parser for interpreting tool-calling output, which is crucial for handling structured JSON responses.  \n",
    "`--chat-template`: Points to the specific Jinja template file. This template formats the conversation history in a way that the model understands for generating tool-call responses. You can get this Jinja template by cloning the vllm-project: `git clone https://github.com/vllm-project/vllm.git`\n",
    "\n",
    "Finally, start the server as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6f016-1418-438d-a0c6-65be5984f7d3",
   "metadata": {},
   "source": [
    "`vllm serve meta-llama/Llama-3.1-8B-Instruct \\`  \n",
    "`--enable-auto-tool-choice \\`  \n",
    "`--tool-call-parser llama3_json \\`  \n",
    "`--port 8088 \\`  \n",
    "`-chat-template vllm/examples/tool_chat_template_llama3.1_json.jinja`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f328f7d",
   "metadata": {},
   "source": [
    "### Step 2: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab557f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set these to the correct values for your setup\n",
    "os.environ[\"VLLM_MODEL\"] = \"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:8088/v1\" # vLLM serve URL (we used port 8089 here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852aa3",
   "metadata": {},
   "source": [
    "### Step 3: Define a tool (`create_burger_order`)\n",
    "\n",
    "LLMs are great at **deciding** what to do, but tools are how they actually **do** it.  \n",
    "The `create_burger_order` tool turns the model’s intent into a deterministic, auditable action.\n",
    "\n",
    "The tool takes the burger items the user wants, generates a unique order ID, and builds a clean, structured order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78b43ba4-9782-412f-b714-6e4be2221b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "from crewai.tools import tool\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    status: Literal[\"input_required\", \"completed\", \"error\"] = \"input_required\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: int\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    order_items: list[OrderItem]\n",
    "\n",
    "\n",
    "@tool(\"create_order\")\n",
    "def create_burger_order(order_items: list[OrderItem]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new burger order with the given order items.\n",
    "\n",
    "    Args:\n",
    "        order_items: List of order items to be added to the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the order has been created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        order_id = str(uuid.uuid4())\n",
    "        order = Order(order_id=order_id, status=\"created\", order_items=order_items)\n",
    "        print(\"===\")\n",
    "        print(f\"order created: {order}\")\n",
    "        print(\"===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating order: {e}\")\n",
    "        return f\"Error creating order: {e}\"\n",
    "    return f\"Order {order.model_dump()} has been created\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd3ee2",
   "metadata": {},
   "source": [
    "### Step 4: Define BurgerSellerAgent class  \n",
    "\n",
    "Previously we mentioned the standard CrewAI agent structure:\n",
    "\n",
    "***Agent → Task → Crew → Output***\n",
    "\n",
    "Now this BurgerSellerAgent class is essentially a wrapper around the Agent-Task-Crew pipeline. It constructs the Agent + Task + Crew. \n",
    "\n",
    "The class BurgerSellerAgent defines the core intelligence and business rules of the burger store agent. It:\n",
    "\n",
    "**- Encapsulates** the instructions, context, and rules the LLM must follow (menu, pricing, confirmation flow, error handling).  \n",
    "**- Connects** the LLM (via CrewAI) with the create_burger_order tool so that order creation is deterministic and safe.  \n",
    "**- Produces** a structured response format (ResponseFormat) that downstream systems (like the A2A server) can reliably consume.  \n",
    "**- Handles** different states (input_required, error, completed) and translates them into a uniform response for the server.  \n",
    "**- Acts** as the bridge between user queries and the A2A server, so that the server doesn’t need to know the conversation logic, i.e it just hosts the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "972af52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent as crewagent\n",
    "from crewai import Crew as crewcrew\n",
    "from crewai import LLM as crewllm\n",
    "from crewai import Task as crewtask\n",
    "from crewai import Process as crewprocess\n",
    "\n",
    "class BurgerSellerAgent:\n",
    "    TaskInstruction = \"\"\"\n",
    "# INSTRUCTIONS\n",
    "\n",
    "You are a specialized assistant for a burger store.\n",
    "Your sole purpose is to answer questions about what is available on burger menu and price also handle order creation.\n",
    "If the user asks about anything other than burger menu or order creation, politely state that you cannot help with that topic and can only assist with burger menu and order creation.\n",
    "Do not attempt to answer unrelated questions or use tools for other purposes.\n",
    "\n",
    "# CONTEXT\n",
    "\n",
    "Received user query: {user_prompt}\n",
    "Session ID: {session_id}\n",
    "\n",
    "Provided below is the available burger menu and it's related price:\n",
    "- Classic Cheeseburger: IDR 85K\n",
    "- Double Cheeseburger: IDR 110K\n",
    "- Spicy Chicken Burger: IDR 80K\n",
    "- Spicy Cajun Burger: IDR 85K\n",
    "\n",
    "# RULES\n",
    "\n",
    "- If user want to do something, you will be following this order:\n",
    "    1. Always ensure the user already confirmed the order and total price. This confirmation may already given in the user query.\n",
    "    2. Use `create_burger_order` tool to create the order\n",
    "    3. Finally, always provide response to the user about the detailed ordered items, price breakdown and total, and order ID\n",
    "    \n",
    "- Set response status to input_required if asking for user order confirmation.\n",
    "- Set response status to error if there is an error while processing the request.\n",
    "- Set response status to completed if the request is complete.\n",
    "- DO NOT make up menu or price, Always rely on the provided menu given to you as context.\n",
    "\"\"\"\n",
    "    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"]\n",
    "\n",
    "    def invoke(self, query, sessionId) -> str:\n",
    "        burger_agent = crewagent(\n",
    "            role=\"Burger Seller Agent\",\n",
    "            goal=(\n",
    "                \"Help user to understand what is available on burger menu and price also handle order creation.\"\n",
    "            ),\n",
    "            backstory=(\"You are an expert and helpful burger seller agent.\"),\n",
    "            verbose=False,\n",
    "            allow_delegation=False,\n",
    "            tools=[create_burger_order],\n",
    "            llm=crewllm(\n",
    "                model=\"hosted_vllm/meta-llama/Llama-3.1-8B-Instruct\", # os.getenv(\"VLLM_MODEL\"), #VLLM_MODEL\n",
    "                api_base=\"http://localhost:8089/v1\" # os.getenv(\"OPENAI_API_BASE\") # OPENAI_API_BASE\n",
    "                )\n",
    "        )\n",
    "\n",
    "        agent_task = crewtask(\n",
    "            description=self.TaskInstruction,\n",
    "            output_pydantic=ResponseFormat,\n",
    "            agent=burger_agent,\n",
    "            expected_output=(\n",
    "                \"A JSON object with 'status' and 'message' fields.\"\n",
    "                \"Set response status to input_required if asking for user order confirmation.\"\n",
    "                \"Set response status to error if there is an error while processing the request.\"\n",
    "                \"Set response status to completed if the request is complete.\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        crew = crewcrew(\n",
    "            tasks=[agent_task],\n",
    "            agents=[burger_agent],\n",
    "            verbose=False,\n",
    "            process=crewprocess.sequential,\n",
    "        )\n",
    "\n",
    "        inputs = {\"user_prompt\": query, \"session_id\": sessionId}\n",
    "        response = crew.kickoff(inputs)\n",
    "        return self.get_agent_response(response)\n",
    "\n",
    "    def get_agent_response(self, response):\n",
    "        response_object = response.pydantic\n",
    "        if response_object and isinstance(response_object, ResponseFormat):\n",
    "            if response_object.status == \"input_required\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"error\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "            elif response_object.status == \"completed\":\n",
    "                return {\n",
    "                    \"is_task_complete\": True,\n",
    "                    \"require_user_input\": False,\n",
    "                    \"content\": response_object.message,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"is_task_complete\": False,\n",
    "            \"require_user_input\": True,\n",
    "            \"content\": \"We are unable to process your request at the moment. Please try again.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d8a4e",
   "metadata": {},
   "source": [
    "### Step 5: Start the Burger Agent A2A Server\n",
    "\n",
    "We now launch the **Burger Agent as an A2A server.**\n",
    "This wraps the CrewAI pipeline **(Agent → Task → Crew → Output)** inside the A2AServer, so the agent is exposed as a discoverable **A2A service**.\n",
    "\n",
    "Running it in a background thread allows the server to:\n",
    "\n",
    "- Continuously listen for incoming requests from other agents (like the Root Agent)  \n",
    "- Keep the Jupyter notebook responsive for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa71119a-a558-4042-9d8d-598d2ddb6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.server import A2AServer\n",
    "from utils.a2a_types import AgentCard, AgentCapabilities, AgentSkill, AgentAuthentication\n",
    "from utils.push_notification_auth import PushNotificationSenderAuth\n",
    "from utils.task_manager import AgentTaskManager\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "AUTH_USERNAME=\"burgeruser123\"\n",
    "AUTH_PASSWORD=\"burgerpass123\"\n",
    "\n",
    "def main(host, port):   \n",
    "\n",
    "    \"\"\"Starts the Burger Seller Agent server.\"\"\"\n",
    "    try:\n",
    "        capabilities = AgentCapabilities(pushNotifications=True)\n",
    "        skill = AgentSkill(\n",
    "            id=\"create_burger_order\",\n",
    "            name=\"Burger Order Creation Tool\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            tags=[\"burger order creation\"],\n",
    "            examples=[\"I want to order 2 classic cheeseburgers\"],\n",
    "        )\n",
    "        agent_card = AgentCard(\n",
    "            name=\"burger_seller_agent\",\n",
    "            description=\"Helps with creating burger orders\",\n",
    "            # The URL provided here is for the sake of demo,\n",
    "            # in production you should use a proper domain name\n",
    "            url=f\"http://{host}:{port}/\",\n",
    "            version=\"1.0.0\",\n",
    "            authentication=AgentAuthentication(schemes=[\"Basic\"]),\n",
    "            defaultInputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            defaultOutputModes=BurgerSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            capabilities=capabilities,\n",
    "            skills=[skill],\n",
    "        )\n",
    "\n",
    "\n",
    "        notification_sender_auth = PushNotificationSenderAuth()\n",
    "        notification_sender_auth.generate_jwk()\n",
    "        server = A2AServer(\n",
    "            agent_card=agent_card,\n",
    "            task_manager=AgentTaskManager(\n",
    "                agent=BurgerSellerAgent(),\n",
    "                notification_sender_auth=notification_sender_auth,\n",
    "            ),\n",
    "            host=host,\n",
    "            port=port,\n",
    "            auth_username=AUTH_USERNAME,\n",
    "            auth_password=AUTH_PASSWORD,\n",
    "        )\n",
    "\n",
    "        server.app.add_route(\n",
    "            \"/.well-known/jwks.json\",\n",
    "            notification_sender_auth.handle_jwks_endpoint,\n",
    "            methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting server on {host}:{port}\")\n",
    "        server.start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during server startup: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11693ea",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "- We define the host (`0.0.0.0`) and a unique port (`10001`) for the Burger Agent. Once started, this agent is ready to respond to A2A requests from the Purchasing Agent.\n",
    "\n",
    "#### Why use a thread?  \n",
    "Running the server in a background thread means:\n",
    "- You can keep interacting with the notebook\n",
    "- Other agents or client components can still be started in additional cells\n",
    "\n",
    "This is essential for working with **multi-agent systems** in an interactive environment like Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "869e7532-cc6e-4f83-a457-d1cdb7214342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server thread started. Waiting a moment for server to initialize on http://0.0.0.0:10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting server on 0.0.0.0:10001\n",
      "INFO:     Started server process [2680652]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:10001 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Global variable to hold the server thread reference ---\n",
    "# This allows you to stop it later from another cell if needed\n",
    "global server_thread\n",
    "server_thread = None\n",
    "\n",
    "# --- Main execution in the Jupyter cell ---\n",
    "if server_thread is not None and server_thread.is_alive():\n",
    "    print(\"Server is already running.\")\n",
    "else:\n",
    "    # Define host and port\n",
    "    server_host = \"0.0.0.0\"\n",
    "    server_port = 10001\n",
    "\n",
    "    # Create and start the thread\n",
    "    server_thread = threading.Thread(target=main, args=(server_host, server_port))\n",
    "    server_thread.daemon = True # Allows the main program to exit even if the thread is still running\n",
    "    server_thread.start()\n",
    "\n",
    "    print(f\"Server thread started. Waiting a moment for server to initialize on http://{server_host}:{server_port}\")\n",
    "    time.sleep(5) # Give it a few seconds to boot up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c790e",
   "metadata": {},
   "source": [
    "### Step 6: Test the Burger Seller Agent\n",
    "\n",
    "Now we can send a sample request to the Burger Agent endpoint and check that:\n",
    "\n",
    "- A well-structured response is returned  \n",
    "- The agent offers burger deals\n",
    "\n",
    "This confirms the agent is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a130f4d-ce41-4aae-9022-20bba37460e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:36:13 - LiteLLM:INFO\u001b[0m: utils.py:3224 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = hosted_vllm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BurgerSellerAgent object at 0x7f1eb05ad640>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.460136 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.789151 seconds\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/llms/openai/openai.py:725\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    724\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    726\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/llms/openai/openai.py:653\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    639\u001b[39m logging_obj.pre_call(\n\u001b[32m    640\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    641\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    647\u001b[39m     },\n\u001b[32m    648\u001b[39m )\n\u001b[32m    650\u001b[39m (\n\u001b[32m    651\u001b[39m     headers,\n\u001b[32m    652\u001b[39m     response,\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:149\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/llms/openai/openai.py:471\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/llms/openai/openai.py:453\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1135\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1134\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/openai/_base_client.py:1014\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1013\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\n\u001b[32m   1017\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1018\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     response.headers,\n\u001b[32m   1023\u001b[39m )\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/main.py:1969\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1963\u001b[39m     logging.post_call(\n\u001b[32m   1964\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   1965\u001b[39m         api_key=api_key,\n\u001b[32m   1966\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   1967\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   1968\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1969\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1972\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/main.py:1942\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1941\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m     response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1950\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   1956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   1958\u001b[39m \u001b[43m        \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1962\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/llms/openai/openai.py:736\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params)\u001b[39m\n\u001b[32m    735\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    737\u001b[39m     status_code=status_code,\n\u001b[32m    738\u001b[39m     message=error_text,\n\u001b[32m    739\u001b[39m     headers=error_headers,\n\u001b[32m    740\u001b[39m     body=error_body,\n\u001b[32m    741\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Connection error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m agent = BurgerSellerAgent()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(agent) \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1 classic cheeseburger pls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault_session\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mBurgerSellerAgent.invoke\u001b[39m\u001b[34m(self, query, sessionId)\u001b[39m\n\u001b[32m     69\u001b[39m crew = crewcrew(\n\u001b[32m     70\u001b[39m     tasks=[agent_task],\n\u001b[32m     71\u001b[39m     agents=[burger_agent],\n\u001b[32m     72\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     73\u001b[39m     process=crewprocess.sequential,\n\u001b[32m     74\u001b[39m )\n\u001b[32m     76\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33muser_prompt\u001b[39m\u001b[33m\"\u001b[39m: query, \u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m: sessionId}\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m response = \u001b[43mcrew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_agent_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/crew.py:669\u001b[39m, in \u001b[36mCrew.kickoff\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_crew_planning()\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.sequential:\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.hierarchical:\n\u001b[32m    671\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_hierarchical_process()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/crew.py:780\u001b[39m, in \u001b[36mCrew._run_sequential_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CrewOutput:\n\u001b[32m    779\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/crew.py:883\u001b[39m, in \u001b[36mCrew._execute_tasks\u001b[39m\u001b[34m(self, tasks, start_index, was_replayed)\u001b[39m\n\u001b[32m    880\u001b[39m     futures.clear()\n\u001b[32m    882\u001b[39m context = \u001b[38;5;28mself\u001b[39m._get_context(task, task_outputs)\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m task_output = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBaseTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools_for_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m task_outputs.append(task_output)\n\u001b[32m    889\u001b[39m \u001b[38;5;28mself\u001b[39m._process_task_result(task, task_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/task.py:356\u001b[39m, in \u001b[36mTask.execute_sync\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sync\u001b[39m(\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    351\u001b[39m     agent: Optional[BaseAgent] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    352\u001b[39m     context: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    353\u001b[39m     tools: Optional[List[BaseTool]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    354\u001b[39m ) -> TaskOutput:\n\u001b[32m    355\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/task.py:504\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28mself\u001b[39m.end_time = datetime.datetime.now()\n\u001b[32m    503\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e), task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/task.py:420\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m.processed_by_agents.add(agent.role)\n\u001b[32m    419\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context=context, task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m pydantic_output, json_output = \u001b[38;5;28mself\u001b[39m._export_output(result)\n\u001b[32m    427\u001b[39m task_output = TaskOutput(\n\u001b[32m    428\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    429\u001b[39m     description=\u001b[38;5;28mself\u001b[39m.description,\n\u001b[32m   (...)\u001b[39m\u001b[32m    435\u001b[39m     output_format=\u001b[38;5;28mself\u001b[39m._get_output_format(),\n\u001b[32m    436\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agent.py:462\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m    454\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    455\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    456\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    460\u001b[39m         ),\n\u001b[32m    461\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    463\u001b[39m \u001b[38;5;28mself\u001b[39m._times_executed += \u001b[32m1\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._times_executed > \u001b[38;5;28mself\u001b[39m.max_retry_limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agent.py:438\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    434\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._execute_with_timeout(\n\u001b[32m    435\u001b[39m             task_prompt, task, \u001b[38;5;28mself\u001b[39m.max_execution_time\n\u001b[32m    436\u001b[39m         )\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_without_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    441\u001b[39m     \u001b[38;5;66;03m# Propagate TimeoutError without retry\u001b[39;00m\n\u001b[32m    442\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    443\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    444\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m         ),\n\u001b[32m    449\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agent.py:534\u001b[39m, in \u001b[36mAgent._execute_without_timeout\u001b[39m\u001b[34m(self, task_prompt, task)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_without_timeout\u001b[39m(\u001b[38;5;28mself\u001b[39m, task_prompt: \u001b[38;5;28mstr\u001b[39m, task: Task) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    525\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute a task without a timeout.\u001b[39;00m\n\u001b[32m    526\u001b[39m \n\u001b[32m    527\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    532\u001b[39m \u001b[33;03m        The output of the agent.\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mask_for_human_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:114\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.ask_for_human_input = \u001b[38;5;28mbool\u001b[39m(inputs.get(\u001b[33m\"\u001b[39m\u001b[33mask_for_human_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     formatted_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    117\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:208\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_context_length_exceeded(e):\n\u001b[32m    210\u001b[39m         handle_context_length(\n\u001b[32m    211\u001b[39m             respect_context_window=\u001b[38;5;28mself\u001b[39m.respect_context_window,\n\u001b[32m    212\u001b[39m             printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m             i18n=\u001b[38;5;28mself\u001b[39m._i18n,\n\u001b[32m    217\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:154\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     formatted_answer = handle_max_iterations_exceeded(\n\u001b[32m    144\u001b[39m         formatted_answer,\n\u001b[32m    145\u001b[39m         printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m    150\u001b[39m     )\n\u001b[32m    152\u001b[39m enforce_rpm_limit(\u001b[38;5;28mself\u001b[39m.request_within_rpm_limit)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m answer = \u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m formatted_answer = process_llm_response(answer, \u001b[38;5;28mself\u001b[39m.use_stop_words)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Extract agent fingerprint if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:160\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[39m\n\u001b[32m    153\u001b[39m     answer = llm.call(\n\u001b[32m    154\u001b[39m         messages,\n\u001b[32m    155\u001b[39m         callbacks=callbacks,\n\u001b[32m    156\u001b[39m         from_task=from_task,\n\u001b[32m    157\u001b[39m         from_agent=from_agent,\n\u001b[32m    158\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[32m    162\u001b[39m     printer.print(\n\u001b[32m    163\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mReceived None or empty response from LLM call.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    164\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    165\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/utilities/agent_utils.py:153\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     answer = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/llm.py:971\u001b[39m, in \u001b[36mLLM.call\u001b[39m\u001b[34m(self, messages, tools, callbacks, available_functions, from_task, from_agent)\u001b[39m\n\u001b[32m    967\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_streaming_response(\n\u001b[32m    968\u001b[39m             params, callbacks, available_functions, from_task, from_agent\n\u001b[32m    969\u001b[39m         )\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_non_streaming_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_agent\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LLMContextLengthExceededException:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Re-raise LLMContextLengthExceededException as it should be handled\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# by the CrewAgentExecutor._invoke_loop method, which can then decide\u001b[39;00m\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# whether to summarize the content or abort based on the respect_context_window flag\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/crewai/llm.py:781\u001b[39m, in \u001b[36mLLM._handle_non_streaming_response\u001b[39m\u001b[34m(self, params, callbacks, available_functions, from_task, from_agent)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;66;03m# --- 1) Make the completion call\u001b[39;00m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# Attempt to make the completion call, but catch context window errors\u001b[39;00m\n\u001b[32m    778\u001b[39m     \u001b[38;5;66;03m# and convert them to our own exception type for consistent handling\u001b[39;00m\n\u001b[32m    779\u001b[39m     \u001b[38;5;66;03m# across the codebase. This allows CrewAgentExecutor to handle context\u001b[39;00m\n\u001b[32m    780\u001b[39m     \u001b[38;5;66;03m# length issues appropriately.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# Convert litellm's context window error to our own exception type\u001b[39;00m\n\u001b[32m    784\u001b[39m     \u001b[38;5;66;03m# for consistent handling in the rest of the codebase\u001b[39;00m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMContextLengthExceededException(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/utils.py:1306\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1303\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1304\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1305\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/utils.py:1181\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1179\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1180\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1184\u001b[39m     kwargs=kwargs,\n\u001b[32m   1185\u001b[39m     call_type=call_type,\n\u001b[32m   1186\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/main.py:3430\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3428\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3429\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3433\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2293\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2292\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2293\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2294\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2295\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/lab_test/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:501\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m500\u001b[39m:\n\u001b[32m    500\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InternalServerError(\n\u001b[32m    502\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInternalServerError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    503\u001b[39m         model=model,\n\u001b[32m    504\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    505\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    506\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    507\u001b[39m     )\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m503\u001b[39m:\n\u001b[32m    509\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mInternalServerError\u001b[39m: litellm.InternalServerError: InternalServerError: Hosted_vllmException - Connection error."
     ]
    }
   ],
   "source": [
    "agent = BurgerSellerAgent()\n",
    "print(agent) \n",
    "result = agent.invoke(\"1 classic cheeseburger pls\", \"default_session\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49046124",
   "metadata": {},
   "source": [
    "### **Remote Seller 2: Pizza Agent**\n",
    "\n",
    "This agent is built with **LangGraph**, and its core **LLM** is served via **Ollama**. . It presents the pizza menu, provides pricing, and handles order creation through the A2A protocol. LangGraph creates the agent as a ReAct agent graph, implementing a cyclical pattern of Reasoning, Acting, and Observing\n",
    "\n",
    "In LangGraph, the ReAct pattern enables \n",
    "- **Reason**: The agent analyzes customer requests and menu options\n",
    "- **Act**: Execute actions by calling appropriate tools\n",
    "- **Observe**: Process the results to provide structured responses.\n",
    "\n",
    "Also, in this example we’ll create our custom tool, `create_pizza_order`, to handle deterministic order creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5ec63-8a7d-4eb1-81b4-341fd068bfeb",
   "metadata": {},
   "source": [
    "### Step 1: Serve a model with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596442a-8ac0-4853-9b1d-6e914859b980",
   "metadata": {},
   "source": [
    "To do this, you will need to download and install Ollama. You can do that, in a separate terminal, with a simple command: `curl -fsSL https://ollama.com/install.sh | sh` and after that, simply execute:  \n",
    "`ollama run llama3.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e48e15",
   "metadata": {},
   "source": [
    "### Step 2: Define a tool (`create_pizza_order`)\n",
    "\n",
    "LLMs are great at understanding user intent and figuring out what actions to take, but as they don’t actually execute those actions themselves we use tools.\n",
    "\n",
    "The `create_pizza_order` tool takes a list of pizza items the user wants, generates a unique order ID, and builds a structured Order object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bec9e-2630-447b-b907-e5cc5d23a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Respond to the user in this format.\"\"\"\n",
    "\n",
    "    status: Literal[\"input_required\", \"completed\", \"error\"] = \"input_required\"\n",
    "    message: str\n",
    "\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    name: str\n",
    "    quantity: int\n",
    "    price: int\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    order_id: str\n",
    "    status: str\n",
    "    order_items: list[OrderItem]\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_pizza_order(order_items: list[OrderItem]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a new pizza order with the given order items.\n",
    "\n",
    "    Args:\n",
    "        order_items: List of order items to be added to the order.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating that the order has been created.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        order_id = str(uuid.uuid4())\n",
    "        order = Order(order_id=order_id, status=\"created\", order_items=order_items)\n",
    "        print(\"===\")\n",
    "        print(f\"order created: {order}\")\n",
    "        print(\"===\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating order: {e}\")\n",
    "        return f\"Error creating order: {e}\"\n",
    "    return f\"Order {order.model_dump()} has been created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b7137",
   "metadata": {},
   "source": [
    "### Step 3: Define PizzaSellerAgent class  \n",
    "\n",
    "As mentioned before LangGraph creates the agent as a ReAct agent graph.\n",
    "\n",
    "This PizzaSellerAgent class is a wrapper around a ReAct graph and it constructs the Agent.\n",
    "\n",
    "The class PizzaSellerAgent defines the core intelligence and business rules of the pizza store agent. It:\n",
    "\n",
    "**- Encapsulates** the instructions, context, and rules the LLM must follow (menu, pricing, confirmation flow, error handling).  \n",
    "**- Connects** the LLM (via LangChain) with the create_pizza_order tool so that order creation is deterministic and safe.  \n",
    "**- Produces** a structured response format (ResponseFormat) that downstream systems (like the A2A server) can reliably consume.  \n",
    "**- Handles** different states (input_required, error, completed) and translates them into a uniform response for the server.  \n",
    "**- Acts** as the bridge between user queries and the A2A server, so that the server doesn’t need to know the conversation logic-it just hosts the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PizzaSellerAgent:\n",
    "    SYSTEM_INSTRUCTION = \"\"\"\n",
    "# INSTRUCTIONS\n",
    "\n",
    "You are a specialized assistant for a pizza store.\n",
    "Your sole purpose is to answer questions about what is available on pizza menu and price also handle order creation.\n",
    "If the user asks about anything other than pizza menu or order creation, politely state that you cannot help with that topic and can only assist with pizza menu and order creation.\n",
    "Do not attempt to answer unrelated questions or use tools for other purposes.\n",
    "\n",
    "# CONTEXT\n",
    "\n",
    "Provided below is the available pizza menu and it's related price:\n",
    "- Margherita Pizza: IDR 100K\n",
    "- Pepperoni Pizza: IDR 140K\n",
    "- Hawaiian Pizza: IDR 110K\n",
    "- Veggie Pizza: IDR 100K\n",
    "- BBQ Chicken Pizza: IDR 130K\n",
    "\n",
    "# RULES\n",
    "\n",
    "- If user want to do something, you will be following this order:\n",
    "    1. Always ensure the user already confirmed the order and total price. This confirmation may already given in the user query.\n",
    "    2. Use `create_pizza_order` tool to create the order\n",
    "    3. Finally, always provide response to the user about the detailed ordered items, price breakdown and total, and order ID\n",
    "\n",
    "- Set response status to input_required if asking for user order confirmation.\n",
    "- Set response status to error if there is an error while processing the request.\n",
    "- Set response status to completed if the request is complete.\n",
    "- DO NOT make up menu or price, Always rely on the provided menu given to you as context.\n",
    "\"\"\"\n",
    "    SUPPORTED_CONTENT_TYPES = [\"text\", \"text/plain\"]\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = ChatOllama(\n",
    "            model=\"llama3.1:latest\" \n",
    "        )\n",
    "\n",
    "        self.tools = [create_pizza_order]\n",
    "        self.graph = create_react_agent(\n",
    "            self.model,\n",
    "            tools=self.tools,\n",
    "            checkpointer=memory,\n",
    "            prompt=self.SYSTEM_INSTRUCTION,\n",
    "            response_format=ResponseFormat,\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, sessionId) -> str:\n",
    "        config = {\"configurable\": {\"thread_id\": sessionId}}\n",
    "        self.graph.invoke({\"messages\": [(\"user\", query)]}, config)\n",
    "        return self.get_agent_response(config)\n",
    "\n",
    "    def get_agent_response(self, config):\n",
    "        current_state = self.graph.get_state(config)\n",
    "        structured_response = current_state.values.get(\"structured_response\")\n",
    "        if structured_response and isinstance(structured_response, ResponseFormat):\n",
    "            if structured_response.status == \"input_required\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "            elif structured_response.status == \"error\":\n",
    "                return {\n",
    "                    \"is_task_complete\": False,\n",
    "                    \"require_user_input\": True,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "            elif structured_response.status == \"completed\":\n",
    "                return {\n",
    "                    \"is_task_complete\": True,\n",
    "                    \"require_user_input\": False,\n",
    "                    \"content\": structured_response.message,\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"is_task_complete\": False,\n",
    "            \"require_user_input\": True,\n",
    "            \"content\": \"We are unable to process your request at the moment. Please try again.\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e490f",
   "metadata": {},
   "source": [
    "### Step 4: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set these to the correct values for your setup\n",
    "os.environ[\"API_KEY\"] = \"pizza123\"\n",
    "os.environ[\"OLLAMA_MODEL\"] = \"ollama_chat/llama3.1:latest\"\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ac5e0",
   "metadata": {},
   "source": [
    "###  Step 5: Start the Pizza Agent Server\n",
    "\n",
    "We now launch the **Pizza Agent as an A2A server.**\n",
    "This wraps the Agent graph inside the A2AServer, so the agent is exposed as a discoverable **A2A service**.\n",
    "\n",
    "Running it in a background thread allows the server to:\n",
    "\n",
    "- Continuously listen for incoming requests from other agents (like the Root Agent)\n",
    "\n",
    "- Keep the Jupyter notebook responsive for further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c25c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.server import A2AServer\n",
    "from utils.a2a_types import AgentCard, AgentCapabilities, AgentSkill, AgentAuthentication\n",
    "from utils.push_notification_auth import PushNotificationSenderAuth\n",
    "from utils.task_manager import AgentTaskManager\n",
    "import click\n",
    "import logging\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main(host, port):\n",
    "    \"\"\"Starts the Pizza Seller Agent server.\"\"\"\n",
    "    try:\n",
    "        capabilities = AgentCapabilities(pushNotifications=True)\n",
    "        skill = AgentSkill(\n",
    "            id=\"create_pizza_order\",\n",
    "            name=\"Pizza Order Creation Tool\",\n",
    "            description=\"Helps with creating pizza orders\",\n",
    "            tags=[\"pizza order creation\"],\n",
    "            examples=[\"I want to order 2 pepperoni pizzas\"],\n",
    "        )\n",
    "        agent_card = AgentCard(\n",
    "            name=\"pizza_seller_agent\",\n",
    "            description=\"Helps with creating pizza orders\",\n",
    "            # The URL provided here is for the sake of demo,\n",
    "            # in production you should use a proper domain name\n",
    "            url=f\"http://{host}:{port}/\",\n",
    "            version=\"1.0.0\",\n",
    "            authentication=AgentAuthentication(schemes=[\"Bearer\"]),\n",
    "            defaultInputModes=PizzaSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            defaultOutputModes=PizzaSellerAgent.SUPPORTED_CONTENT_TYPES,\n",
    "            capabilities=capabilities,\n",
    "            skills=[skill],\n",
    "        )\n",
    "\n",
    "        notification_sender_auth = PushNotificationSenderAuth()\n",
    "        notification_sender_auth.generate_jwk()\n",
    "        server = A2AServer(\n",
    "            agent_card=agent_card,\n",
    "            task_manager=AgentTaskManager(\n",
    "                agent=PizzaSellerAgent(),\n",
    "                notification_sender_auth=notification_sender_auth,\n",
    "            ),\n",
    "            host=host,\n",
    "            port=port,\n",
    "            api_key=os.environ.get(\"API_KEY\"),\n",
    "        )\n",
    "\n",
    "        server.app.add_route(\n",
    "            \"/.well-known/jwks.json\",\n",
    "            notification_sender_auth.handle_jwks_endpoint,\n",
    "            methods=[\"GET\"],\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting server on {host}:{port}\")\n",
    "        server.start()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during server startup: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c526a07",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "- We define the host (`0.0.0.0`) and a unique port (`10000`) for the Pizza Agent. Once started, this agent is ready to respond to A2A requests from the Purchasing Agent.\n",
    "\n",
    "#### Why use a thread?\n",
    "Running the server in a background thread means:\n",
    "- You can keep interacting with the notebook\n",
    "- Other agents or client components can still be started in additional cells\n",
    "\n",
    "This is essential for working with **multi-agent systems** in an interactive environment like Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18594cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global server_thread\n",
    "server_thread = None\n",
    "\n",
    "# --- Main execution in the Jupyter cell ---\n",
    "if server_thread is not None and server_thread.is_alive():\n",
    "    print(\"Server is already running.\")\n",
    "else:\n",
    "    # Define host and port\n",
    "    server_host = \"0.0.0.0\"\n",
    "    server_port = 10000\n",
    "\n",
    "    # Create and start the thread\n",
    "    server_thread = threading.Thread(target=main, args=(server_host, server_port))\n",
    "    server_thread.daemon = True # Allows the main program to exit even if the thread is still running\n",
    "    server_thread.start()\n",
    "\n",
    "    print(f\"Server thread started. Waiting a moment for server to initialize on http://{server_host}:{server_port}\")\n",
    "    time.sleep(5) # Give it a few seconds to boot up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec95a8",
   "metadata": {},
   "source": [
    "### Step 6: Test the Pizza Seller Agent\n",
    "\n",
    "Now we can send a sample request to the Pizza Agent endpoint and check that:\n",
    "\n",
    "- A well-structured response is returned\n",
    "- The agent offers pizza deals\n",
    "\n",
    "This confirms the agent is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PizzaSellerAgent()\n",
    "print(agent) \n",
    "result = agent.invoke(\"I want to order 2 pepperoni pizzas\", \"default_session\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d86ea4",
   "metadata": {},
   "source": [
    "## **Root Agent: Purchasing Concierge (Google ADK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36892a2a",
   "metadata": {},
   "source": [
    "This agent is built with **Google ADK** and it's core LLM is served via **Ollama**, It coordinates orders by delegating tasks to these seller agents through the open A2A protocol.  \n",
    "As we create the agent in ADK without using Google or Gemini models, we use LiteLLM to create the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf18405",
   "metadata": {},
   "source": [
    "### Step 1: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"OLLAMA_MODEL\"] = \"ollama_chat/llama3.1:latest\"\n",
    "os.environ[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "os.environ[\"PIZZA_SELLER_AGENT_AUTH\"] = \"pizza123\"\n",
    "os.environ[\"PIZZA_SELLER_AGENT_URL\"] = \"http://localhost:10000\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_AUTH\"] = \"burgeruser123:burgerpass123\"\n",
    "os.environ[\"BURGER_SELLER_AGENT_URL\"] = \"http://localhost:10001\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb99be",
   "metadata": {},
   "source": [
    "### Step 2: Remote Agent Connections\n",
    "\n",
    "The root agent will delegate tasks to remote agents through the A2A protocol.\n",
    "\n",
    "The RemoteAgentConnections class is a communication wrapper that manages interactions with remote agents in the A2A ecosystem. It\n",
    "\n",
    "- **Establishes** A2A client connections with agent-specific authentication (API keys, basic auth).\n",
    "- **Sends** tasks to remote agents via send_task() method with callback support for real-time updates.\n",
    "- **Manages** metadata propagation and message ID tracking to maintain conversation continuity.\n",
    "- **Handles** task lifecycle events (status updates, artifact changes) through callback functions.\n",
    "- **Maintains** session state with conversation context and pending task tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03826e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import uuid\n",
    "from utils.a2a_types import (\n",
    "    AgentCard,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TaskStatusUpdateEvent,\n",
    "    TaskArtifactUpdateEvent,\n",
    ")\n",
    "from utils.client import A2AClient\n",
    "import os\n",
    "\n",
    "TaskCallbackArg = Task | TaskStatusUpdateEvent | TaskArtifactUpdateEvent\n",
    "TaskUpdateCallback = Callable[[TaskCallbackArg, AgentCard], Task]\n",
    "\n",
    "KNOWN_AUTH = {\n",
    "    \"pizza_seller_agent\": os.getenv(\"PIZZA_SELLER_AGENT_AUTH\", \"api_key\"),\n",
    "    \"burger_seller_agent\": os.getenv(\"BURGER_SELLER_AGENT_AUTH\", \"user:pass\"),\n",
    "}\n",
    "\n",
    "\n",
    "class RemoteAgentConnections:\n",
    "    \"\"\"A class to hold the connections to the remote agents.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_card: AgentCard, agent_url: str):\n",
    "        auth = KNOWN_AUTH.get(agent_card.name, None)\n",
    "        self.agent_client = A2AClient(agent_card, auth=auth, agent_url=agent_url)\n",
    "        self.card = agent_card\n",
    "\n",
    "        self.conversation_name = None\n",
    "        self.conversation = None\n",
    "        self.pending_tasks = set()\n",
    "\n",
    "    def get_agent(self) -> AgentCard:\n",
    "        return self.card\n",
    "\n",
    "    async def send_task(\n",
    "        self,\n",
    "        request: TaskSendParams,\n",
    "        task_callback: TaskUpdateCallback | None,\n",
    "    ) -> Task | None:\n",
    "        response = await self.agent_client.send_task(request.model_dump())\n",
    "        merge_metadata(response.result, request)\n",
    "        # For task status updates, we need to propagate metadata and provide\n",
    "        # a unique message id.\n",
    "        if (\n",
    "            hasattr(response.result, \"status\")\n",
    "            and hasattr(response.result.status, \"message\")\n",
    "            and response.result.status.message\n",
    "        ):\n",
    "            merge_metadata(response.result.status.message, request.message)\n",
    "            m = response.result.status.message\n",
    "            if not m.metadata:\n",
    "                m.metadata = {}\n",
    "            if \"message_id\" in m.metadata:\n",
    "                m.metadata[\"last_message_id\"] = m.metadata[\"message_id\"]\n",
    "            m.metadata[\"message_id\"] = str(uuid.uuid4())\n",
    "\n",
    "        if task_callback:\n",
    "            task_callback(response.result, self.card)\n",
    "        return response.result\n",
    "\n",
    "\n",
    "def merge_metadata(target, source):\n",
    "    if not hasattr(target, \"metadata\") or not hasattr(source, \"metadata\"):\n",
    "        return\n",
    "    if target.metadata and source.metadata:\n",
    "        target.metadata.update(source.metadata)\n",
    "    elif source.metadata:\n",
    "        target.metadata = dict(**source.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf4245",
   "metadata": {},
   "source": [
    "### Step 3: Define PurchasingAgent class \n",
    "\n",
    "As mentioned ealier we create Agent with non-google or gemini model using LiteLLM. \n",
    "\n",
    "Now this PurchasingAgent class is a wrapper around AgentCard and Agent components.\n",
    "\n",
    "The class PurchasingAgent defines the core orchestration and delegation logic. It:\n",
    "- **Discovers** remote agents by resolving Agent Cards, establishing authenticated connections, and maintaining a seller agent registry.\n",
    "- **Orchestrates** task delegation through intelligent routing, session management, and asynchronous task handling\n",
    "- **Manage** responses by aggregating seller outputs, formatting user messages, and handling state transitions.\n",
    "- **Controls** conversation flow by tracking agent assignments, managing metadata propagation, and coordinating multi-agent interactions.\n",
    "- **Provides** reliability through error handling, clean session management, and structured response validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "from google.adk.models.lite_llm import LiteLlm \n",
    "from google.adk import Agent\n",
    "from google.adk.agents.readonly_context import ReadonlyContext\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from utils.card_resolver import A2ACardResolver\n",
    "from utils.a2a_types import (\n",
    "    AgentCard,\n",
    "    Message,\n",
    "    TaskState,\n",
    "    Task,\n",
    "    TaskSendParams,\n",
    "    TextPart,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "\n",
    "class PurchasingAgent:\n",
    "    \"\"\"The purchasing agent.\n",
    "\n",
    "    This is the agent responsible for choosing which remote seller agents to send\n",
    "    tasks to and coordinate their work.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        remote_agent_addresses: List[str],\n",
    "        task_callback: TaskUpdateCallback | None = None,\n",
    "    ):\n",
    "        self.task_callback = task_callback\n",
    "        self.remote_agent_connections: dict[str, RemoteAgentConnections] = {}\n",
    "        self.cards: dict[str, AgentCard] = {}\n",
    "        for address in remote_agent_addresses:\n",
    "            card_resolver = A2ACardResolver(address)\n",
    "            try:\n",
    "                card = card_resolver.get_agent_card()\n",
    "                # The URL accessed here should be the same as the one provided in the agent card\n",
    "                # However, in this demo we are using the URL provided in the key arguments\n",
    "                remote_connection = RemoteAgentConnections(\n",
    "                    agent_card=card, agent_url=address\n",
    "                )\n",
    "                self.remote_agent_connections[card.name] = remote_connection\n",
    "                self.cards[card.name] = card\n",
    "            except httpx.ConnectError:\n",
    "                print(f\"ERROR: Failed to get agent card from : {address}\")\n",
    "        agent_info = []\n",
    "        for ra in self.list_remote_agents():\n",
    "            agent_info.append(json.dumps(ra))\n",
    "        self.agents = \"\\n\".join(agent_info)\n",
    "\n",
    "    def create_agent(self) -> Agent:\n",
    "        return Agent(\n",
    "            model=LiteLlm(model=os.getenv(\"OLLAMA_MODEL\")), \n",
    "            name=\"purchasing_agent\",\n",
    "            instruction=self.root_instruction,\n",
    "            before_model_callback=self.before_model_callback,\n",
    "            description=(\n",
    "                \"This purchasing agent orchestrates the decomposition of the user purchase request into\"\n",
    "                \" tasks that can be performed by the seller agents.\"\n",
    "            ),\n",
    "            tools=[\n",
    "                self.send_task,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def root_instruction(self, context: ReadonlyContext) -> str:\n",
    "        current_agent = self.check_active_agent(context)\n",
    "        return f\"\"\"You are an expert purchasing delegator that can delegate the user product inquiry and purchase request to the\n",
    "appropriate seller remote agents.\n",
    "\n",
    "Execution:\n",
    "- For actionable tasks, you can use `send_task` to assign tasks to remote agents to perform.\n",
    "- When the remote agent is repeatedly asking for user confirmation, assume that the remote agent doesn't have access to user's conversation context. \n",
    "    So improve the task description to include all the necessary information related to that agent\n",
    "- Never ask user permission when you want to connect with remote agents. If you need to make connection with multiple remote agents, directly\n",
    "    connect with them without asking user permission or asking user preference\n",
    "- Always show the detailed response information from the seller agent and propagate it properly to the user. \n",
    "- If the remote seller is asking for confirmation, rely the confirmation question to the user if the user haven't do so. \n",
    "- If the user already confirmed the related order in the past conversation history, you can confirm on behalf of the user\n",
    "- Do not give irrelevant context to remote seller agent. For example, ordered pizza item is not relevant for the burger seller agent\n",
    "- Never ask order confirmation to the remote seller agent \n",
    "\n",
    "Please rely on tools to address the request, and don't make up the response. If you are not sure, please ask the user for more details.\n",
    "Focus on the most recent parts of the conversation primarily.\n",
    "\n",
    "If there is an active agent, send the request to that agent with the update task tool.\n",
    "\n",
    "Agents:\n",
    "{self.agents}\n",
    "\n",
    "Current active seller agent: {current_agent[\"active_agent\"]}\n",
    "\"\"\"\n",
    "\n",
    "    def check_active_agent(self, context: ReadonlyContext):\n",
    "        state = context.state\n",
    "        if (\n",
    "            \"session_id\" in state\n",
    "            and \"session_active\" in state\n",
    "            and state[\"session_active\"]\n",
    "            and \"active_agent\" in state\n",
    "        ):\n",
    "            return {\"active_agent\": f\"{state['active_agent']}\"}\n",
    "        return {\"active_agent\": \"None\"}\n",
    "\n",
    "    def before_model_callback(self, callback_context: CallbackContext, llm_request):\n",
    "        state = callback_context.state\n",
    "        if \"session_active\" not in state or not state[\"session_active\"]:\n",
    "            if \"session_id\" not in state:\n",
    "                state[\"session_id\"] = str(uuid.uuid4())\n",
    "            state[\"session_active\"] = True\n",
    "\n",
    "    def list_remote_agents(self):\n",
    "        \"\"\"List the available remote agents you can use to delegate the task.\"\"\"\n",
    "        if not self.remote_agent_connections:\n",
    "            return []\n",
    "\n",
    "        remote_agent_info = []\n",
    "        for card in self.cards.values():\n",
    "            print(f\"Found agent card: {card.model_dump()}\")\n",
    "            print(\"=\" * 100)\n",
    "            remote_agent_info.append(\n",
    "                {\"name\": card.name, \"description\": card.description}\n",
    "            )\n",
    "        return remote_agent_info\n",
    "\n",
    "    async def send_task(self, agent_name: str, task: str, tool_context: ToolContext):\n",
    "        \"\"\"Sends a task to remote seller agent\n",
    "\n",
    "        This will send a message to the remote agent named agent_name.\n",
    "\n",
    "        Args:\n",
    "            agent_name: The name of the agent to send the task to.\n",
    "            task: The comprehensive conversation context summary\n",
    "                and goal to be achieved regarding user inquiry and purchase request.\n",
    "            tool_context: The tool context this method runs in.\n",
    "\n",
    "        Yields:\n",
    "            A dictionary of JSON data.\n",
    "        \"\"\"\n",
    "        if agent_name not in self.remote_agent_connections:\n",
    "            raise ValueError(f\"Agent {agent_name} not found\")\n",
    "        state = tool_context.state\n",
    "        state[\"active_agent\"] = agent_name\n",
    "        client = self.remote_agent_connections[agent_name]\n",
    "        if not client:\n",
    "            raise ValueError(f\"Client not available for {agent_name}\")\n",
    "        if \"task_id\" in state:\n",
    "            taskId = state[\"task_id\"]\n",
    "        else:\n",
    "            taskId = str(uuid.uuid4())\n",
    "        sessionId = state[\"session_id\"]\n",
    "        task: Task\n",
    "        messageId = \"\"\n",
    "        metadata = {}\n",
    "        if \"input_message_metadata\" in state:\n",
    "            metadata.update(**state[\"input_message_metadata\"])\n",
    "            if \"message_id\" in state[\"input_message_metadata\"]:\n",
    "                messageId = state[\"input_message_metadata\"][\"message_id\"]\n",
    "        if not messageId:\n",
    "            messageId = str(uuid.uuid4())\n",
    "        metadata.update(**{\"conversation_id\": sessionId, \"message_id\": messageId})\n",
    "        request: TaskSendParams = TaskSendParams(\n",
    "            id=taskId,\n",
    "            sessionId=sessionId,\n",
    "            message=Message(\n",
    "                role=\"user\",\n",
    "                parts=[TextPart(text=task)],\n",
    "                metadata=metadata,\n",
    "            ),\n",
    "            acceptedOutputModes=[\"text\", \"text/plain\"],\n",
    "            # pushNotification=None,\n",
    "            metadata={\"conversation_id\": sessionId},\n",
    "        )\n",
    "        task = await client.send_task(request, self.task_callback)\n",
    "        # Assume completion unless a state returns that isn't complete\n",
    "        state[\"session_active\"] = task.status.state not in [\n",
    "            TaskState.COMPLETED,\n",
    "            TaskState.CANCELED,\n",
    "            TaskState.FAILED,\n",
    "            TaskState.UNKNOWN,\n",
    "        ]\n",
    "        if task.status.state == TaskState.INPUT_REQUIRED:\n",
    "            # Force user input back\n",
    "            tool_context.actions.escalate = True\n",
    "        elif task.status.state == TaskState.COMPLETED:\n",
    "            # Reset active agent is task is completed\n",
    "            state[\"active_agent\"] = \"None\"\n",
    "\n",
    "        response = []\n",
    "        if task.status.message:\n",
    "            # Assume the information is in the task message.\n",
    "            response.extend(convert_parts(task.status.message.parts, tool_context))\n",
    "        if task.artifacts:\n",
    "            for artifact in task.artifacts:\n",
    "                response.extend(convert_parts(artifact.parts, tool_context))\n",
    "        return response\n",
    "\n",
    "\n",
    "def convert_parts(parts: list[Part], tool_context: ToolContext):\n",
    "    rval = []\n",
    "    for p in parts:\n",
    "        rval.append(convert_part(p, tool_context))\n",
    "    return rval\n",
    "\n",
    "\n",
    "def convert_part(part: Part, tool_context: ToolContext):\n",
    "    # Currently only support text parts\n",
    "    if part.type == \"text\":\n",
    "        return part.text\n",
    "\n",
    "    return f\"Unknown type: {part.type}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_agent = PurchasingAgent(\n",
    "    remote_agent_addresses=[\n",
    "        os.getenv(\"PIZZA_SELLER_AGENT_URL\", \"http://localhost:10000\"),\n",
    "        os.getenv(\"BURGER_SELLER_AGENT_URL\", \"http://localhost:10001\"),\n",
    "    ]\n",
    ").create_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a46ebd",
   "metadata": {},
   "source": [
    "### Final Step: Run the Purchasing Concierge Agent with a UI\n",
    "\n",
    "To make it easier to test the agent, we’ve built a **simple Gradio interface**.\n",
    "\n",
    "- It runs in your browser at: **http://localhost:8084** (local)\n",
    "- #### Try Asking for a Menu!\n",
    "\n",
    "    Once all the agents are running, you can test their responses by asking:\n",
    "\n",
    "    > “Can I see the menu?”  \n",
    "\n",
    "    > or  \n",
    "\n",
    "    > “What pizza options do you have today?” / “What burgers are on the menu?”\n",
    "\n",
    "    Each seller agent (Burger & Pizza) will reply with their available items and prices.  \n",
    "    This is a great way to test if the agents are reachable and responding properly.\n",
    "\n",
    "\n",
    "\n",
    "This is helpful for testing how the full multi-agent system works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f31045",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Dict, Any\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.events import Event\n",
    "from typing import AsyncIterator\n",
    "from google.genai import types\n",
    "from pprint import pformat\n",
    "\n",
    "purchasing_agent = root_agent\n",
    "\n",
    "APP_NAME = \"purchasing_concierge_app\"\n",
    "USER_ID = \"default_user\"\n",
    "SESSION_ID = \"default_session\"\n",
    "SESSION_SERVICE = InMemorySessionService()\n",
    "PURCHASING_AGENT_RUNNER = Runner(\n",
    "    agent=purchasing_agent,  # The agent we want to run\n",
    "    app_name=APP_NAME,  # Associates runs with our app\n",
    "    session_service=SESSION_SERVICE,  # Uses our session manager\n",
    ")\n",
    "SESSION_SERVICE.create_session(\n",
    "    app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    ")\n",
    "\n",
    "\n",
    "async def get_response_from_agent(\n",
    "    message: str,\n",
    "    history: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    \"\"\"Send the message to the backend and get a response.\n",
    "\n",
    "    Args:\n",
    "        message: Text content of the message.\n",
    "        history: List of previous message dictionaries in the conversation.\n",
    "\n",
    "    Returns:\n",
    "        Text response from the backend service.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    events_iterator: AsyncIterator[Event] = PURCHASING_AGENT_RUNNER.run_async(\n",
    "        user_id=USER_ID,\n",
    "        session_id=SESSION_ID,\n",
    "        new_message=types.Content(role=\"user\", parts=[types.Part(text=message)]),\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    async for event in events_iterator:  # event has type Event\n",
    "        if event.content.parts:\n",
    "            for part in event.content.parts:\n",
    "                if part.function_call:\n",
    "                    formatted_call = f\"```python\\n{pformat(part.function_call.model_dump(), indent=2, width=80)}\\n```\"\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=f\"{part.function_call.name}:\\n{formatted_call}\",\n",
    "                            metadata={\"title\": \"🛠️ Tool Call\"},\n",
    "                        )\n",
    "                    )\n",
    "                elif part.function_response:\n",
    "                    formatted_response = f\"```python\\n{pformat(part.function_response.model_dump(), indent=2, width=80)}\\n```\"\n",
    "\n",
    "                    responses.append(\n",
    "                        gr.ChatMessage(\n",
    "                            role=\"assistant\",\n",
    "                            content=formatted_response,\n",
    "                            metadata={\"title\": \"⚡ Tool Response\"},\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # Key Concept: is_final_response() marks the concluding message for the turn\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                # Extract text from the first part\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            elif event.actions and event.actions.escalate:\n",
    "                # Handle potential errors/escalations\n",
    "                final_response_text = (\n",
    "                    f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "                )\n",
    "            responses.append(\n",
    "                gr.ChatMessage(role=\"assistant\", content=final_response_text)\n",
    "            )\n",
    "            yield responses\n",
    "            break  # Stop processing events once the final response is found\n",
    "\n",
    "        yield responses\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = gr.ChatInterface(\n",
    "        get_response_from_agent,\n",
    "        title=\"Purchasing Concierge\",\n",
    "        description=\"This assistant can help you to purchase food from remote sellers.\",\n",
    "        type=\"messages\",\n",
    "    )\n",
    "\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=8084,\n",
    "        share=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1e502-1cc8-4179-a8bf-da61f0a3581d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a9ab9-f472-4f2b-bcb9-08c2ce4b576d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf147622-7c5a-48d9-a933-303899fe7a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce44131-b797-4fde-a799-67ba9dc37a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b256300-5c94-4e84-b327-5bff80a94290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27c776-f2c4-4b06-88dd-3af6b6ee7b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lab_test] *",
   "language": "python",
   "name": "conda-env-lab_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
